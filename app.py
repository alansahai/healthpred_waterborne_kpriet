"""
Health Outbreak Early Warning System - Operational Mode
Real-time prediction, alerting, and monitoring dashboard
Focus: Auto-predict ‚Üí Auto-alert ‚Üí Operational monitoring
"""
import streamlit as st
import pandas as pd
import geopandas as gpd
import folium
from streamlit_folium import st_folium
import plotly.graph_objects as go
import os
import numpy as np
import json
from datetime import datetime

from model import OutbreakPredictor
from utils import get_risk_color
from utils.constants import (
    ALERT_ELEVATED_MAX,
    ALERT_NORMAL_MAX,
    ALERT_LOAD_ELEVATED_MAX,
    ALERT_LOAD_NORMAL_MAX,
    ARTIFACT_VERSION_LABEL,
    CONTRIBUTING_FACTOR_THRESHOLDS,
    FORECAST_HORIZON_DAYS,
    GLOBAL_THRESHOLD,
    INTERVENTION_SIMULATION_REDUCTION_PCT,
    MODERATE_RISK_RATIO,
    OPERATIONAL_MODE_LABEL,
)
from utils.geo_mapping import aggregate_predictions_to_zones, map_ward_to_zone
from utils.runtime_config import load_runtime_config
from utils.spatial_viz import (
    compute_peripheral_indicator,
    compute_infection_influence_arrows,
    compute_neighbor_risk_overlay,
    get_spatial_viz_summary,
    load_ward_centroids,
    simulate_spatial_spread,
    get_spread_simulation_summary,
)


# =============================================================================
# PAGE CONFIGURATION & STYLING
# =============================================================================

st.set_page_config(
    page_title="Disease Outbreak Early Warning System",
    page_icon="üè•",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .stMetric {
        background-color: var(--secondary-background-color);
        padding: 10px;
        border-radius: 5px;
        border: 1px solid rgba(128, 128, 128, 0.25);
    }
</style>
""", unsafe_allow_html=True)


# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def resolve_default_data_path():
    """Resolve best operational data source with integrated dataset priority."""
    config = load_runtime_config()
    data_dir = os.path.join(os.path.dirname(__file__), 'data')
    preferred = [
        'data/integrated_surveillance_dataset_final.csv',
        'data/integrated_surveillance_dataset.csv',
        config.get('data_path', 'data/coimbatore_unified_master_dataset_with_zone.csv'),
        'data/coimbatore_unified_master_dataset_with_zone.csv',
    ]

    for candidate in preferred:
        full_candidate = os.path.join(os.path.dirname(__file__), candidate)
        if os.path.exists(full_candidate):
            return candidate

    if not os.path.isdir(data_dir):
        raise FileNotFoundError('data directory not found')
    
    csv_candidates = sorted([f for f in os.listdir(data_dir) if f.lower().endswith('.csv')])
    if not csv_candidates:
        raise FileNotFoundError('No CSV files found in data directory')
    
    return f"data/{csv_candidates[0]}"


@st.cache_resource
def load_predictor():
    """Load trained model from artifact (uses runtime config for model path)"""
    try:
        from utils.runtime_config import load_runtime_config
        runtime_cfg = load_runtime_config()
        model_path = runtime_cfg.get('model_path', 'model/final_outbreak_model_v3.pkl')
        full_model_path = os.path.join(os.path.dirname(__file__), model_path)
        if not os.path.exists(full_model_path):
            raise RuntimeError(f"Model artifact missing: {model_path}")
        predictor = OutbreakPredictor(model_path=model_path)
        predictor.load_model()
        return predictor, None
    except Exception as e:
        return None, str(e)


@st.cache_data
def load_data(path: str):
    """Load dataset from CSV"""
    try:
        full_path = os.path.join(os.path.dirname(__file__), path)
        df = pd.read_csv(full_path)
        return df, None
    except Exception as e:
        return None, str(e)


def get_alert_status_color(alert_load: float):
    """Interpret alert load for operations command center."""
    if alert_load < ALERT_NORMAL_MAX:
        return "üü¢", "Normal"
    elif alert_load < ALERT_ELEVATED_MAX:
        return "üü°", "Elevated"
    else:
        return "üî¥", "Emergency"


def get_prediction_history_path() -> str:
    return os.path.join(os.path.dirname(__file__), 'data', 'prediction_history.json')


def load_prediction_history() -> list:
    history_path = get_prediction_history_path()
    if not os.path.exists(history_path):
        try:
            with open(history_path, 'w', encoding='utf-8') as handle:
                json.dump([], handle, indent=2)
        except OSError:
            return []
        return []

    try:
        with open(history_path, 'r', encoding='utf-8') as handle:
            payload = json.load(handle)
        return payload if isinstance(payload, list) else []
    except (json.JSONDecodeError, OSError, TypeError):
        return []


def append_prediction_history_entry(current_date: str, high_risk_count: int, alert_load: float, alert_status: str):
    history = load_prediction_history()
    entry = {
        'date': current_date,
        'high_risk_wards': int(high_risk_count),
        'alert_load_pct': round(float(alert_load) * 100.0, 2),
        'alert_status': str(alert_status),
    }

    if history:
        last = history[-1]
        if (
            last.get('date') == entry['date']
            and int(last.get('high_risk_wards', -1)) == entry['high_risk_wards']
            and float(last.get('alert_load_pct', -1)) == entry['alert_load_pct']
            and str(last.get('alert_status', '')) == entry['alert_status']
        ):
            return history

    history.append(entry)
    history_path = get_prediction_history_path()
    try:
        with open(history_path, 'w', encoding='utf-8') as handle:
            json.dump(history, handle, indent=2)
    except OSError:
        return history

    return history


def calculate_alert_load(predictions_df: pd.DataFrame, threshold: float) -> float:
    """Calculate percentage of wards above threshold"""
    if len(predictions_df) == 0:
        return 0.0
    high_risk = len(predictions_df[predictions_df['probability'] >= threshold])
    return high_risk / len(predictions_df)


def render_data_fusion_summary(data: pd.DataFrame):
    """Show visible Health + Water + Rainfall ‚Üí Integrated dataset summary."""
    if data is None or len(data) == 0:
        st.info("Data Fusion Summary unavailable (no data loaded).")
        return

    weeks = int(data['week_start_date'].nunique()) if 'week_start_date' in data.columns else 0
    wards = int(data['ward_id'].nunique()) if 'ward_id' in data.columns else 0
    date_start = str(pd.to_datetime(data['week_start_date']).min().date()) if 'week_start_date' in data.columns else 'N/A'
    date_end = str(pd.to_datetime(data['week_start_date']).max().date()) if 'week_start_date' in data.columns else 'N/A'

    st.markdown("### üîó Data Fusion Summary")
    st.caption("Health Data + Water Quality + Rainfall ‚Üí Integrated Weekly Surveillance Dataset")
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Health Signal", "‚úÖ")
    col2.metric("Water Signal", "‚úÖ")
    col3.metric("Rainfall Signal", "‚úÖ")
    col4.metric("Integrated Rows", f"{len(data):,}")

    st.caption(f"Coverage: {weeks} weeks | {wards} wards | {date_start} to {date_end}")
    preview_cols = [
        'week_start_date',
        'ward_id',
        'reported_cases',
        'turbidity',
        'ecoli_index',
        'rainfall_mm',
        'zone',
    ]
    preview_cols = [col for col in preview_cols if col in data.columns]
    if preview_cols:
        st.dataframe(data[preview_cols].head(8), use_container_width=True, hide_index=True)


def render_data_freshness_coverage_status(data: pd.DataFrame, data_loaded_at: datetime | None = None):
    """Display latest-week freshness and ward coverage diagnostics."""
    st.markdown("### Data Freshness & Coverage Status")

    if data is None or len(data) == 0:
        st.warning("Data Freshness & Coverage unavailable: dataset is empty.")
        return

    if 'week_start_date' not in data.columns:
        st.warning("Data Freshness & Coverage unavailable: week_start_date column missing.")
        return

    working = data[['week_start_date']].copy()
    working['week_start_date'] = pd.to_datetime(working['week_start_date'], errors='coerce')
    if working['week_start_date'].isna().all():
        st.warning("Data Freshness & Coverage unavailable: week_start_date values are invalid.")
        return

    latest_week = working['week_start_date'].max()
    latest_mask = pd.to_datetime(data['week_start_date'], errors='coerce') == latest_week
    latest_week_df = data.loc[latest_mask]

    expected_wards = 100
    if 'ward_id' in data.columns:
        total_unique_wards = int(data['ward_id'].nunique())
        if total_unique_wards > 0:
            expected_wards = total_unique_wards

    present_wards = int(latest_week_df['ward_id'].nunique()) if 'ward_id' in latest_week_df.columns else 0
    missing_wards = max(0, expected_wards - present_wards)
    completeness = (present_wards / expected_wards * 100.0) if expected_wards > 0 else 0.0
    loaded_at_text = (
        data_loaded_at.strftime('%Y-%m-%d %H:%M:%S')
        if isinstance(data_loaded_at, datetime)
        else datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    )

    col1, col2, col3 = st.columns(3)
    col1.metric("Latest Surveillance Week", latest_week.strftime('%Y-%m-%d'))
    col2.metric("Ward Coverage", f"{present_wards} / {expected_wards}")
    col3.metric("Missing Wards", missing_wards)

    col4, col5 = st.columns(2)
    col4.metric("Completeness", f"{completeness:.1f}%")
    col5.metric("Data Loaded At", loaded_at_text)


# =============================================================================
# DATA INTEGRATION FUNCTIONS
# =============================================================================

def load_health_data():
    """
    Load health surveillance data (hospital reports)
    Currently loads from static CSV, structured as if from live source
    """
    try:
        base_dir = os.path.dirname(__file__)
        health = pd.read_csv(os.path.join(base_dir, 'data', 'health_surveillance_weekly.csv'))
        wards = pd.read_csv(os.path.join(base_dir, 'data', 'ward_metadata.csv'))[['ward_id', 'zone']]
        health = health.merge(wards, on='ward_id', how='left')
        return health, None
    except Exception as e:
        return None, f"Health data load error: {str(e)}"


def load_water_data():
    """
    Load water quality data (turbidity, E.coli, contamination)
    Currently loads from static CSV, structured as if from monitoring stations
    """
    try:
        base_dir = os.path.dirname(__file__)
        water = pd.read_csv(os.path.join(base_dir, 'data', 'water_quality_weekly.csv'))
        wards = pd.read_csv(os.path.join(base_dir, 'data', 'ward_metadata.csv'))[['ward_id', 'zone']]
        water = water.merge(wards, on='ward_id', how='left')
        return water, None
    except Exception as e:
        return None, f"Water data load error: {str(e)}"


def load_rainfall_data():
    """
    Load rainfall data (precipitation amounts, monsoon patterns)
    Currently loads from static CSV, structured as if from weather stations
    """
    try:
        base_dir = os.path.dirname(__file__)
        rainfall = pd.read_csv(os.path.join(base_dir, 'data', 'rainfall_environment_weekly.csv'))
        wards = pd.read_csv(os.path.join(base_dir, 'data', 'ward_metadata.csv'))[['ward_id', 'zone']]
        rainfall = rainfall.merge(wards, on='ward_id', how='left')
        return rainfall, None
    except Exception as e:
        return None, f"Rainfall data load error: {str(e)}"


def integrate_data(health_df, water_df, rain_df):
    """
    Integrate health surveillance, water quality, and rainfall data
    Performs merges and validation
    
    Returns:
    - integrated_df: Merged dataframe
    - integration_report: Dictionary with integration metrics
    """
    if health_df is None or water_df is None or rain_df is None:
        return None, {"error": "One or more data sources unavailable"}
    
    try:
        expected_wards = int(health_df['ward_id'].nunique())
        expected_weeks = int(pd.to_datetime(health_df['week_start_date']).nunique())
        expected_rows = int(expected_wards * expected_weeks)

        # Merge on week_start_date + ward_id (zone carried from metadata)
        merged = health_df.merge(water_df, on=['week_start_date', 'ward_id', 'zone'], how='inner')
        if len(merged) != expected_rows:
            raise ValueError(
                f"Row loss after health-water merge: expected {expected_rows}, got {len(merged)}"
            )

        merged = merged.merge(rain_df, on=['week_start_date', 'ward_id', 'zone'], how='inner')
        if len(merged) != expected_rows:
            raise ValueError(
                f"Row loss after adding rainfall merge: expected {expected_rows}, got {len(merged)}"
            )
        
        # Calculate integration metrics
        report = {
            'total_rows': len(merged),
            'unique_wards': merged['ward_id'].nunique(),
            'unique_zones': merged['zone'].nunique(),
            'date_range_start': pd.to_datetime(merged['week_start_date']).min() if len(merged) > 0 else None,
            'date_range_end': pd.to_datetime(merged['week_start_date']).max() if len(merged) > 0 else None,
            'missing_health': health_df.isnull().sum().to_dict() if health_df is not None else {},
            'missing_water': water_df.isnull().sum().to_dict() if water_df is not None else {},
            'missing_rain': rain_df.isnull().sum().to_dict() if rain_df is not None else {},
            'missing_integrated': merged.isnull().sum().to_dict(),
            'health_rows': len(health_df) if health_df is not None else 0,
            'water_rows': len(water_df) if water_df is not None else 0,
            'rain_rows': len(rain_df) if rain_df is not None else 0,
        }
        
        return merged, report
    except Exception as e:
        return None, {"error": f"Integration failed: {str(e)}"}


# =============================================================================
# PREVENTION & INTERVENTION FUNCTIONS
# =============================================================================

def get_contributing_factors(predictor, ward_data: pd.DataFrame) -> list:
    """
    Extract top 3 contributing features for a ward's high risk prediction.
    Uses rule-based scoring: raw feature values, trend signals (growth rates), 
    and rolling averages without SHAP or heavy explainability.
    """
    try:
        if predictor is None or ward_data is None or len(ward_data) == 0:
            return []
        
        row = ward_data.iloc[0]
        signals = []
        
        # SIGNAL 1: E. coli contamination (raw + trend)
        if 'ecoli_index' in ward_data.columns and pd.notna(row['ecoli_index']):
            ecoli_val = row['ecoli_index']
            ecoli_avg_col = 'ecoli_2w_avg'
            ecoli_growth = None
            if ecoli_avg_col in ward_data.columns:
                avg = row.get(ecoli_avg_col, None)
                if pd.notna(avg):
                    ecoli_growth = ecoli_val - avg if pd.notna(avg) else None
            
            threshold = CONTRIBUTING_FACTOR_THRESHOLDS['ecoli_index']
            if ecoli_val > threshold or (ecoli_growth and ecoli_growth > 0.5):
                severity = 'High' if ecoli_val > threshold * 1.5 else 'Moderate'
                detail = 'Spike detected' if ecoli_growth and ecoli_growth > 0.5 else 'Elevated'
                signals.append({
                    'feature': 'E. coli contamination',
                    'value': ecoli_val,
                    'detail': detail,
                    'severity': severity,
                    'score': ecoli_val + (ecoli_growth if ecoli_growth and ecoli_growth > 0 else 0)
                })
        
        # SIGNAL 2: Rainfall spike (raw + trend)
        if 'rainfall_mm' in ward_data.columns and pd.notna(row['rainfall_mm']):
            rainfall_val = row['rainfall_mm']
            rainfall_avg_col = 'rainfall_2w_avg'
            rainfall_growth = None
            if rainfall_avg_col in ward_data.columns:
                avg = row.get(rainfall_avg_col, None)
                if pd.notna(avg):
                    rainfall_growth = rainfall_val - avg if pd.notna(avg) else None
            
            threshold = CONTRIBUTING_FACTOR_THRESHOLDS['rainfall_mm']
            if rainfall_val > threshold or (rainfall_growth and rainfall_growth > 10):
                severity = 'High' if rainfall_val > threshold * 1.5 else 'Moderate'
                detail = 'Sharp spike' if rainfall_growth and rainfall_growth > 10 else 'Above normal'
                signals.append({
                    'feature': 'Rainfall spike',
                    'value': rainfall_val,
                    'detail': detail,
                    'severity': severity,
                    'score': rainfall_val + (rainfall_growth if rainfall_growth and rainfall_growth > 0 else 0)
                })
        
        # SIGNAL 3: Rising case trend (growth rate + rolling avg)
        if 'reported_cases' in ward_data.columns and pd.notna(row['reported_cases']):
            cases_val = row['reported_cases']
            cases_avg_col = 'cases_2w_avg'
            cases_growth = None
            if cases_avg_col in ward_data.columns:
                avg = row.get(cases_avg_col, None)
                if pd.notna(avg) and avg > 0:
                    cases_growth = (cases_val - avg) / avg if pd.notna(avg) else None
            
            threshold = CONTRIBUTING_FACTOR_THRESHOLDS['reported_cases']
            if cases_val > threshold or (cases_growth and cases_growth > 0.15):
                severity = 'High' if cases_val > threshold * 1.5 else 'Moderate'
                detail = 'Rising trend detected' if cases_growth and cases_growth > 0.15 else 'Elevated count'
                signals.append({
                    'feature': 'Rising case trend',
                    'value': cases_val,
                    'detail': detail,
                    'severity': severity,
                    'score': cases_val + (cases_growth * 100 if cases_growth and cases_growth > 0 else 0)
                })
        
        # SIGNAL 4: Water turbidity
        if 'turbidity' in ward_data.columns and pd.notna(row['turbidity']):
            turbidity_val = row['turbidity']
            threshold = CONTRIBUTING_FACTOR_THRESHOLDS['turbidity']
            if turbidity_val > threshold:
                severity = 'High' if turbidity_val > threshold * 1.5 else 'Moderate'
                signals.append({
                    'feature': 'High turbidity',
                    'value': turbidity_val,
                    'detail': 'Water clarity compromised',
                    'severity': severity,
                    'score': turbidity_val
                })
        
        # SIGNAL 5: Monsoon season
        if 'monsoon_flag' in ward_data.columns and pd.notna(row['monsoon_flag']):
            if row['monsoon_flag'] == CONTRIBUTING_FACTOR_THRESHOLDS['monsoon_flag']:
                signals.append({
                    'feature': 'Monsoon season',
                    'value': 1.0,
                    'detail': 'Peak outbreak risk window',
                    'severity': 'Moderate',
                    'score': 50  # Monsoon always scores moderate
                })
        
        # Sort by severity (High first) then by score (descending) and return top 3
        signals = sorted(signals, key=lambda x: (x['severity'] != 'High', -x['score']))[:3]
        
        return signals
    except (KeyError, ValueError, TypeError, IndexError):
        return []


def get_action_plan(contributing_factors: list) -> list:
    """
    Map detected risk factors to suggested interventions
    Rule-based approach: high rainfall + high ecoli ‚Üí chlorination, etc.
    """
    actions = []
    
    # Check for specific risk factor combinations
    factor_names = [f['feature'].lower() for f in contributing_factors]
    
    if any('ecoli' in f or 'turbidity' in f for f in factor_names):
        actions.append({
            'action': 'üö∞ Immediate Water Chlorination',
            'description': 'Deploy emergency water treatment to reduce contamination',
            'simulation_effect': ('ecoli_index', INTERVENTION_SIMULATION_REDUCTION_PCT['ecoli_index']),
            'urgency': 'Critical'
        })
    
    if any('rainfall' in f for f in factor_names):
        actions.append({
            'action': 'üè• Deploy Mobile Medical Camp',
            'description': 'Increase water sampling and disease surveillance during wet season',
            'simulation_effect': ('rainfall_mm', INTERVENTION_SIMULATION_REDUCTION_PCT['rainfall_mm']),
            'urgency': 'High'
        })
    
    if any('cases' in f or 'disease' in f for f in factor_names):
        actions.append({
            'action': 'üíä Increase Health Screening',
            'description': 'Deploy additional health camps for early case detection',
            'simulation_effect': ('reported_cases', INTERVENTION_SIMULATION_REDUCTION_PCT['reported_cases']),
            'urgency': 'High'
        })
    
    if any('monsoon' in f for f in factor_names):
        actions.append({
            'action': 'üìä Enhanced Water Quality Monitoring',
            'description': 'Increase sampling frequency to 2x during monsoon season',
            'simulation_effect': ('turbidity', INTERVENTION_SIMULATION_REDUCTION_PCT['turbidity']),
            'urgency': 'Moderate'
        })
    
    # Default action if no specific factors detected
    if not actions:
        actions.append({
            'action': 'üîç Routine Surveillance',
            'description': 'Continue standard monitoring and case documentation',
            'simulation_effect': None,
            'urgency': 'Low'
        })
    
    return actions[:3]  # Return top 3 actions


def simulate_intervention(predictor, ward_data: pd.DataFrame, intervention: dict) -> dict:
    """
    Simulate the effect of an intervention on outbreak probability
    Returns: {'original_prob': float, 'new_prob': float, 'improvement': float}
    """
    try:
        if not intervention.get('simulation_effect') or predictor is None:
            return {'original_prob': 0, 'new_prob': 0, 'improvement': 0, 'error': 'Cannot simulate'}
        
        # Get original prediction
        original_pred = predictor.predict_latest_week(df=ward_data)
        if original_pred is None or len(original_pred) == 0:
            return {'error': 'No prediction available'}
        
        original_prob = original_pred['probability'].iloc[0]
        
        # Apply intervention: reduce feature by specified percentage
        feature_name, reduction_pct = intervention['simulation_effect']
        simulated_data = ward_data.copy().sort_values('week_start_date').reset_index(drop=True)
        
        if feature_name in simulated_data.columns:
            latest_idx = simulated_data.index[-1]
            current_val = simulated_data.loc[latest_idx, feature_name]
            if pd.notna(current_val) and current_val != 0:
                reduction_factor = 1 + (reduction_pct / 100)  # e.g., -40% = multiply by 0.6
                simulated_data.loc[latest_idx, feature_name] = current_val * reduction_factor
        
        # Get new prediction
        new_pred = predictor.predict_latest_week(df=simulated_data)
        if new_pred is None or len(new_pred) == 0:
            return {'error': 'Simulation prediction failed'}
        
        new_prob = new_pred['probability'].iloc[0]
        improvement = original_prob - new_prob
        
        return {
            'original_prob': original_prob,
            'new_prob': new_prob,
            'improvement': improvement,
            'improvement_pct': (improvement / original_prob * 100) if original_prob > 0 else 0
        }
    except Exception as e:
        return {'error': str(e)}


def create_heatmap_with_all_wards(
    predictions_df: pd.DataFrame, 
    gdf: gpd.GeoDataFrame, 
    threshold: float,
    show_influence_arrows: bool = False,
    show_neighbor_overlay: bool = False
):
    """Create risk heatmap ensuring all wards are rendered.
    
    Args:
        predictions_df: DataFrame with ward predictions
        gdf: GeoDataFrame with zone geometries
        threshold: Risk threshold
        show_influence_arrows: If True, draw arrows from high-risk peripheral wards
        show_neighbor_overlay: If True, add intensity overlay based on neighbor risk
    """
    if gdf is None or len(gdf) == 0:
        return folium.Map(location=[11.0168, 76.9558], zoom_start=11)

    # Aggregate predictions to zones
    zone_predictions = aggregate_predictions_to_zones(predictions_df, threshold=threshold)

    # Create map
    projected = gdf.to_crs(epsg=3857)
    centroids_wgs84 = projected.geometry.centroid.to_crs(epsg=4326)
    center_lat = centroids_wgs84.y.mean()
    center_lon = centroids_wgs84.x.mean()

    m = folium.Map(
        location=[center_lat, center_lon],
        zoom_start=11,
        tiles='OpenStreetMap'
    )

    # Compute neighbor overlay if enabled (for border styling)
    neighbor_overlay = {}
    if show_neighbor_overlay:
        neighbor_overlay = compute_neighbor_risk_overlay(predictions_df)

    # Merge predictions with geometry
    gdf_zones = gdf.copy()
    if not zone_predictions.empty:
        gdf_zones = gdf_zones.merge(
            zone_predictions,
            left_on='name',
            right_on='zone',
            how='left'
        )

    # Render all zones (including those without predictions)
    for _, row in gdf_zones.iterrows():
        risk_color = get_risk_color(row.get('risk', 'Low')) if pd.notna(row.get('risk')) else '#cccccc'
        
        tooltip_text = f"<b>Zone:</b> {row['name']}<br>"
        if pd.notna(row.get('avg_probability')):
            tooltip_text += (
                f"<b>Avg Probability:</b> {row['avg_probability']:.2f}<br>"
                f"<b>Wards:</b> {int(row['ward_count'])}<br>"
                f"<b>Risk:</b> {row['risk']}"
            )
        else:
            tooltip_text += "<b>Status:</b> No data"

        folium.GeoJson(
            row['geometry'],
            style_function=lambda x, color=risk_color: {
                'fillColor': color,
                'color': 'black',
                'weight': 2,
                'fillOpacity': 0.6
            },
            tooltip=folium.Tooltip(tooltip_text)
        ).add_to(m)

    # Add influence arrows if enabled
    if show_influence_arrows:
        arrows = compute_infection_influence_arrows(predictions_df, threshold=threshold)
        for arrow in arrows:
            # Draw polyline arrow from high-risk peripheral ward to neighbor
            folium.PolyLine(
                locations=[arrow['from_coords'], arrow['to_coords']],
                color='#FF4500',  # Orange-red for spillover
                weight=2,
                opacity=0.7,
                dash_array='5, 5',  # Dashed line
                tooltip=f"Spillover risk: {arrow['from_ward']} ‚Üí {arrow['to_ward']} (P={arrow['influence']:.2f})"
            ).add_to(m)
            
            # Add small circle at destination to indicate influence target
            folium.CircleMarker(
                location=arrow['to_coords'],
                radius=4,
                color='#FF4500',
                fill=True,
                fillOpacity=0.5,
                tooltip=f"Influenced by {arrow['from_ward']}"
            ).add_to(m)

    # Add neighbor risk intensity markers if enabled
    if show_neighbor_overlay and neighbor_overlay:
        centroids = load_ward_centroids()
        for ward_id, data in neighbor_overlay.items():
            if data['high_risk_neighbor_count'] > 0 and ward_id in centroids:
                coords = (centroids[ward_id]['lat'], centroids[ward_id]['lon'])
                intensity = data['intensity']
                
                # Only show markers for wards with elevated neighbor risk
                if intensity > 0.3:
                    folium.CircleMarker(
                        location=coords,
                        radius=6 + (intensity * 8),  # Size based on intensity
                        color='#8B0000',  # Dark red
                        fill=True,
                        fillColor='#FF6347',  # Tomato
                        fillOpacity=intensity * 0.6,
                        tooltip=f"{ward_id}: {data['high_risk_neighbor_count']} high-risk neighbors (avg: {data['neighbor_avg_risk']:.2f})"
                    ).add_to(m)

    return m


def display_alert_list(predictions_df: pd.DataFrame, threshold: float, max_display: int = 10):
    """Display ranked alert list"""
    high_risk = predictions_df[predictions_df['probability'] >= threshold].sort_values('probability', ascending=False)

    if len(high_risk) == 0:
        st.success("‚úÖ No alerts at this time")
        return

    st.error(f"üö® **{len(high_risk)} WARD(S) ABOVE THRESHOLD** (‚â• {threshold:.2f})")
    st.markdown("**Ranked by outbreak probability (highest first)**")

    display_data = high_risk[['ward_id', 'probability', 'risk']].head(max_display).copy()
    display_data['Zone'] = display_data['ward_id'].apply(map_ward_to_zone)
    display_data['Probability %'] = display_data['probability'].apply(lambda x: f"{x:.1%}")
    display_data = display_data[['ward_id', 'Zone', 'Probability %', 'risk']]
    display_data = display_data.rename(columns={'ward_id': 'Ward', 'risk': 'Risk Level'})

    st.dataframe(display_data, use_container_width=True, hide_index=True)

    if len(high_risk) > max_display:
        st.info(f"Showing top {max_display} of {len(high_risk)} alerts")


# =============================================================================
# PAGE: OPERATIONAL MONITORING (DEFAULT)
# =============================================================================

def page_operational_monitoring():
    """Main operational monitoring page"""
    # Initialize session state for interactive features
    if 'resolved_alerts' not in st.session_state:
        st.session_state.resolved_alerts = set()
    if 'feedback_accurate' not in st.session_state:
        st.session_state.feedback_accurate = 0
    if 'feedback_inaccurate' not in st.session_state:
        st.session_state.feedback_inaccurate = 0
    if 'what_if_enabled' not in st.session_state:
        st.session_state.what_if_enabled = False
    if 'what_if_rainfall' not in st.session_state:
        st.session_state.what_if_rainfall = 50.0
    if 'what_if_turbidity' not in st.session_state:
        st.session_state.what_if_turbidity = 3.0
    if 'what_if_ecoli' not in st.session_state:
        st.session_state.what_if_ecoli = 10.0
    if 'what_if_ward' not in st.session_state:
        st.session_state.what_if_ward = None

    st.title("üè• Disease Outbreak Early Warning System")
    st.markdown("""
    **Operational Mode** ‚Äî Real-time monitoring and alerting system  
    **Predicting outbreak risk 7 days ahead** using trained ML model
    """)

    # =========================================================================
    # AUTO-LOAD: Model and Data
    # =========================================================================

    predictor = None
    data = None
    predictions = None
    model_metadata = None
    data_loaded_at = None
    error_occurred = False

    with st.spinner("‚è≥ Loading trained model..."):
        try:
            predictor, error = load_predictor()
            if error:
                raise RuntimeError(f"Model load failed: {error}")
        except (FileNotFoundError, RuntimeError, ValueError, OSError) as e:
            st.error(f"‚ùå Cannot load model: {str(e)}")
            error_occurred = True

    if not error_occurred and predictor:
        with st.spinner("‚è≥ Loading latest dataset..."):
            try:
                data_path = resolve_default_data_path()
                data, error = load_data(data_path)
                if error:
                    raise ValueError(f"Data load failed: {error}")
                data_loaded_at = datetime.now()
            except (FileNotFoundError, ValueError, pd.errors.ParserError, OSError) as e:
                st.error(f"‚ùå Cannot load data: {str(e)}")
                error_occurred = True

    if not error_occurred and predictor and data is not None:
        st.markdown("---")
        render_data_fusion_summary(data)
        st.markdown("---")
        render_data_freshness_coverage_status(data, data_loaded_at=data_loaded_at)

        # ===================================================================
        # DATA QUALITY MONITOR (Before Predictions)
        # ===================================================================
        
        quality_report = display_data_quality_monitor(data)
        
        # ===================================================================
        # AUTO-PREDICT: Generate predictions for latest week
        # ===================================================================

        try:
            preloaded_metadata = predictor.get_model_metadata()
            data_signature = (
                data_path,
                int(len(data)),
                str(pd.to_datetime(data['week_start_date']).max()) if 'week_start_date' in data.columns and len(data) > 0 else 'N/A',
                str(preloaded_metadata.get('artifact_version', 'N/A')),
                str(preloaded_metadata.get('trained_at_utc', 'N/A')),
            )
        except (KeyError, ValueError, TypeError):
            data_signature = None

        cache_hit = (
            data_signature is not None
            and st.session_state.get('operational_prediction_signature') == data_signature
            and st.session_state.get('operational_predictions_cache') is not None
            and st.session_state.get('operational_model_metadata_cache') is not None
        )

        if cache_hit:
            predictions = st.session_state['operational_predictions_cache']
            model_metadata = st.session_state['operational_model_metadata_cache']
            engineered_df = st.session_state.get('operational_engineered_cache')
        else:
            with st.spinner("‚è≥ Generating predictions..."):
                try:
                    predictions = predictor.predict_latest_week(df=data)
                    model_metadata = predictor.get_model_metadata()
                    engineered_df = predictor.latest_engineered
                    st.session_state['operational_prediction_signature'] = data_signature
                    st.session_state['operational_predictions_cache'] = predictions
                    st.session_state['operational_model_metadata_cache'] = model_metadata
                    st.session_state['operational_engineered_cache'] = engineered_df
                except (ValueError, KeyError, TypeError, RuntimeError) as e:
                    st.error(f"‚ùå Prediction failed: {str(e)}")
                    error_occurred = True
                    engineered_df = None

        # ===================================================================
        # SIDEBAR: WHAT-IF RISK SIMULATOR
        # ===================================================================

        st.sidebar.markdown("---")
        st.sidebar.markdown("### üéØ What-If Risk Simulator")
        st.sidebar.markdown("*Adjust environmental factors to see how risk changes*")

        enable_whatif = st.sidebar.checkbox("Enable What-If Simulation", value=False, key='whatif_toggle')
        
        if enable_whatif:
            st.session_state.what_if_enabled = True
            st.session_state.what_if_ward = st.sidebar.selectbox(
                "Select Ward",
                sorted(data['ward_id'].unique()),
                key='whatif_ward_selector'
            )

            st.sidebar.markdown("**Adjust environmental factors:**")
            
            st.session_state.what_if_rainfall = st.sidebar.slider(
                "Rainfall (mm)",
                min_value=0.0,
                max_value=200.0,
                value=st.session_state.what_if_rainfall,
                step=5.0,
                help="Current week rainfall amount",
                key='whatif_rainfall_slider'
            )

            st.session_state.what_if_turbidity = st.sidebar.slider(
                "Turbidity (NTU)",
                min_value=0.0,
                max_value=20.0,
                value=st.session_state.what_if_turbidity,
                step=0.5,
                help="Water turbidity index",
                key='whatif_turbidity_slider'
            )

            st.session_state.what_if_ecoli = st.sidebar.slider(
                "E. coli Index (CFU/100ml)",
                min_value=0.0,
                max_value=100.0,
                value=st.session_state.what_if_ecoli,
                step=2.0,
                help="E. coli contamination level",
                key='whatif_ecoli_slider'
            )

            # Compute what-if scenario
            if st.session_state.what_if_ward:
                ward_data = data[data['ward_id'] == st.session_state.what_if_ward].copy()
                if len(ward_data) > 0:
                    # Get baseline prediction
                    baseline_pred = predictions[predictions['ward_id'] == st.session_state.what_if_ward]
                    if len(baseline_pred) > 0:
                        baseline_prob = baseline_pred['probability'].iloc[0]
                        baseline_risk = baseline_pred['risk'].iloc[0]
                    else:
                        baseline_prob = 0.0
                        baseline_risk = 'Unknown'

                    st.sidebar.markdown("---")
                    st.sidebar.markdown("**Baseline vs. Scenario:**")
                    
                    col1, col2 = st.sidebar.columns(2)
                    col1.write(f"**{st.session_state.what_if_ward}**")
                    col1.metric("Baseline", f"{baseline_prob:.1%}", label_visibility='collapsed')
                    col1.caption(f"Risk: {baseline_risk}")
                    
                    # Create scenario with adjusted values only on latest row
                    scenario = ward_data.copy().sort_values('week_start_date').reset_index(drop=True)
                    latest_idx = scenario.index[-1]
                    scenario.loc[latest_idx, 'rainfall_mm'] = st.session_state.what_if_rainfall
                    scenario.loc[latest_idx, 'turbidity'] = st.session_state.what_if_turbidity
                    scenario.loc[latest_idx, 'ecoli_index'] = st.session_state.what_if_ecoli
                    
                    # Try to predict scenario
                    try:
                        scenario_pred = predictor.predict_latest_week(df=scenario)
                        if len(scenario_pred) > 0:
                            scenario_prob = scenario_pred['probability'].iloc[0]
                            scenario_risk = scenario_pred['risk'].iloc[0]
                            
                            col2.write("**Scenario**")
                            col2.metric("Adjusted", f"{scenario_prob:.1%}", label_visibility='collapsed')
                            col2.caption(f"Risk: {scenario_risk}")
                            
                            diff = scenario_prob - baseline_prob
                            diff_pct = (diff / baseline_prob * 100) if baseline_prob > 0 else 0
                            
                            st.sidebar.markdown("---")
                            if diff > 0.01:
                                st.sidebar.error(f"üìà Risk increases by {diff:.1%} ({diff_pct:+.0f}%)")
                            elif diff < -0.01:
                                st.sidebar.success(f"üìâ Risk decreases by {abs(diff):.1%} ({diff_pct:.0f}%)")
                            else:
                                st.sidebar.info(f"‚û°Ô∏è Risk essentially unchanged")
                        else:
                            st.sidebar.warning("Could not compute scenario prediction")
                    except (ValueError, KeyError, TypeError):
                        st.sidebar.warning(f"Scenario analysis unavailable")

        else:
            st.session_state.what_if_enabled = False

        # ===================================================================
        # SIDEBAR: FEEDBACK LOOP
        # ===================================================================

        st.sidebar.markdown("---")
        st.sidebar.markdown("### üí¨ Give Feedback")
        st.sidebar.markdown("*Help improve alert confidence*")
        
        col1, col2 = st.sidebar.columns(2)
        if col1.button("‚úÖ Accurate", key='btn_accurate', help='This alert was confirmed'):
            st.session_state.feedback_accurate += 1
            st.sidebar.success("Thank you!")
        if col2.button("‚ùå Inaccurate", key='btn_inaccurate', help='This alert was a false alarm'):
            st.session_state.feedback_inaccurate += 1
            st.sidebar.info("Noted.")
        
        total_feedback = st.session_state.feedback_accurate + st.session_state.feedback_inaccurate
        if total_feedback > 0:
            accuracy_rate = st.session_state.feedback_accurate / total_feedback * 100
            st.sidebar.markdown(f"**Feedback:** {accuracy_rate:.0f}% confirmed ({total_feedback} total)")
        else:
            st.sidebar.caption("No feedback yet")

    # =========================================================================
    # AUTO-ALERT: Display alerts if predictions available
    # =========================================================================

    if not error_occurred and predictions is not None and model_metadata is not None:
        if 'global_threshold' not in model_metadata or model_metadata.get('global_threshold') is None:
            st.error(f"‚ùå Model metadata missing required global_threshold. {OPERATIONAL_MODE_LABEL} halted.")
            return
        threshold = float(model_metadata['global_threshold'])

        if predictor is None:
            st.error("‚ùå Predictor instance unavailable. Operational mode halted.")
            return

        threshold_tolerance = 5e-3
        if abs(threshold - float(GLOBAL_THRESHOLD)) > threshold_tolerance:
            st.error(
                f"‚ùå Threshold policy drift detected: artifact={threshold:.6f}, expected={float(GLOBAL_THRESHOLD):.6f}, "
                f"tolerance=¬±{threshold_tolerance:.6f}. "
                f"{OPERATIONAL_MODE_LABEL} halted."
            )
            return

        if len(predictions) != 100:
            st.error(
                f"‚ùå Prediction cardinality mismatch: got {len(predictions)} wards, expected 100. "
                f"{OPERATIONAL_MODE_LABEL} halted."
            )
            return
        
        # ===================================================================
        # AUTOMATION STATUS: Confirm monitoring flow
        # ===================================================================
        
        with st.expander(f"ü§ñ **Automation Status** (System running in {OPERATIONAL_MODE_LABEL})", expanded=False):
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Model Loaded", "‚úÖ", help="Predictor ready")
            col2.metric("Auto-Predict", "‚úÖ", help="Latest week predictions active")
            col3.metric("Auto-Alert", "‚úÖ", help="Alerts generated automatically")
            col4.metric("Manual Retraining", "Admin Only", help="Training moved to ‚öôÔ∏è Admin panel")
            st.caption(f"‚úì {OPERATIONAL_MODE_LABEL} requires no manual intervention for daily monitoring. Training remains in Administration panel only.")
        
        # ===================================================================
        # ALERT STATUS: Green/Yellow/Red
        # ===================================================================

        st.markdown("---")
        st.markdown("### üö® ALERT SYSTEM")

        alert_load = calculate_alert_load(predictions, threshold)
        icon, status_text = get_alert_status_color(alert_load)

        col1, col2, col3, col4 = st.columns([1, 2, 1, 1])
        with col1:
            st.metric("Alert Status", icon)
        with col2:
            st.markdown(f"**{status_text}**  ({alert_load:.1%} of wards above threshold)")
            st.caption(
                f"Alert Load Interpretation: <{ALERT_NORMAL_MAX:.0%} Normal | "
                f"{ALERT_NORMAL_MAX:.0%}‚Äì{ALERT_ELEVATED_MAX:.0%} Elevated | "
                f"‚â•{ALERT_ELEVATED_MAX:.0%} Emergency"
            )
        with col3:
            st.metric("Decision Threshold", f"{threshold:.3f}")
        with col4:
            # Compute % of high-risk wards that are peripheral (display-only)
            high_risk_df = predictions[predictions['risk_class'] == 'High']
            if 'is_peripheral_ward' in predictions.columns and len(high_risk_df) > 0:
                peripheral_high = high_risk_df[high_risk_df['is_peripheral_ward'] == 1]
                peripheral_pct = (len(peripheral_high) / len(high_risk_df)) * 100
                st.metric("% High-Risk (Peripheral)", f"{peripheral_pct:.1f}%")
            else:
                st.metric("% High-Risk (Peripheral)", "N/A")
        
        st.caption("‚ÑπÔ∏è Model now incorporates neighbor spread risk.")

        # ===================================================================
        # ALERT LIST (RANKED BY PROBABILITY) WITH ACTION TRACKING
        # ===================================================================

        st.markdown("---")
        st.markdown("### üì¢ ACTIVE & RESOLVED ALERTS")
        
        # Display active alerts
        high_risk = predictions[predictions['probability'] >= threshold].sort_values('probability', ascending=False)
        
        if len(high_risk) == 0:
            st.success("‚úÖ No alerts at this time")
        else:
            st.error(f"üö® **{len(high_risk)} WARD(S) ABOVE THRESHOLD** (‚â• {threshold:.2f})")
            st.markdown("**Ranked by outbreak probability (highest first). Click ‚ñº to view risk response & interventions.**")
            
            for idx, row in high_risk.head(10).iterrows():
                # ALERT HEADER ROW
                col1, col2, col3, col4 = st.columns([1.5, 1, 1.5, 1])
                
                with col1:
                    zone = map_ward_to_zone(row['ward_id'])
                    st.write(f"**{row['ward_id']}** ({zone})")
                
                with col2:
                    st.metric("Risk", f"{row['probability']:.1%}", label_visibility='collapsed')
                
                with col3:
                    risk_level = row['risk']
                    if risk_level == 'High':
                        st.error(risk_level)
                    elif risk_level == 'Moderate':
                        st.warning(risk_level)
                    else:
                        st.info(risk_level)
                
                with col4:
                    alert_key = f"alert_{row['ward_id']}"
                    if st.button("‚úì Actioned", key=alert_key, help="Mark this alert as addressed"):
                        st.session_state.resolved_alerts.add(row['ward_id'])
                        st.success(f"‚úì {row['ward_id']} marked as resolved")
                
                # RISK RESPONSE PANEL (Expandable)
                with st.expander(f"üî¥ **Risk Response Panel** ‚Äî {row['ward_id']}", expanded=False):
                    # Get contributing factors (use engineered features if available, fallback to raw data)
                    ward_row = engineered_df[engineered_df['ward_id'] == row['ward_id']] if engineered_df is not None else data[data['ward_id'] == row['ward_id']]
                    contributing = get_contributing_factors(predictor, ward_row)
                    
                    col1, col2 = st.columns([1.5, 1.5])
                    
                    with col1:
                        st.markdown("**üìä Contributing Risk Factors:**")
                        if contributing:
                            for i, factor in enumerate(contributing, 1):
                                severity_emoji = "üî¥" if factor['severity'] == 'High' else "üü°"
                                st.write(f"{severity_emoji} **{i}. {factor['feature']}**")
                                st.caption(f"   {factor['detail']} | Value: {factor['value']:.1f}")
                        else:
                            st.info("No major contributing factors detected")
                    
                    with col2:
                        st.markdown("**üìà Risk Level Details:**")
                        st.metric("Outbreak Probability", f"{row['probability']:.1%}")
                        st.metric("Risk Classification", row['risk'])
                        st.metric("Decision Threshold", f"{threshold:.3f}")
                    
                    st.markdown("---")
                    st.markdown("**üéØ Suggested Action Plan:**")
                    
                    # Get action plan
                    actions = get_action_plan(contributing)
                    
                    for action_idx, action in enumerate(actions):
                        urgency_color = "üî¥" if action['urgency'] == 'Critical' else "üü†" if action['urgency'] == 'High' else "üü°"
                        intervention_type = action['action'].replace('üö∞', '').replace('üè•', '').replace('üíä', '').replace('üìä', '').strip()
                        
                        st.markdown(f"\n{urgency_color} **{action['action']}** *(Urgency: {action['urgency']})*")
                        st.caption(action['description'])
                        st.caption(f"Intervention Type: {intervention_type} | Priority Level: {action['urgency']}")
                        
                        # Intervention simulation button
                        if action['simulation_effect']:
                            sim_key = f"simulate_{row['ward_id']}_{action_idx}"
                            if st.button(f"üìä Simulate: {action['action'].split()[0]}...", key=sim_key):
                                # Store intervention for visualization
                                if f'intervention_{row["ward_id"]}' not in st.session_state:
                                    st.session_state[f'intervention_{row["ward_id"]}'] = {}
                                
                                sim_result = simulate_intervention(predictor, ward_row, action)
                                st.session_state[f'intervention_{row["ward_id"]}'] = {**action, **sim_result}
                        
                        # Show simulation results if available
                        sim_key_check = f'intervention_{row["ward_id"]}'
                        if sim_key_check in st.session_state:
                            sim_data = st.session_state[sim_key_check]
                            if 'new_prob' in sim_data and sim_data.get('improvement') is not None:
                                st.success(
                                    f"‚úÖ **Result:** Risk reduced from {sim_data['original_prob']:.1%} ‚Üí {sim_data['new_prob']:.1%} "
                                    f"({sim_data['improvement_pct']:.0f}% improvement)"
                                )
            
            if len(high_risk) > 10:
                st.info(f"Showing top 10 of {len(high_risk)} alerts")
        
        # Display resolved alerts
        if len(st.session_state.resolved_alerts) > 0:
            st.markdown("---")
            st.markdown("### ‚úÖ RESOLVED ALERTS (ACTION TAKEN)")
            resolved_list = ", ".join(sorted(st.session_state.resolved_alerts))
            st.info(f"Marked as resolved: {resolved_list}")
            
            if st.button("üîÑ Clear resolved alerts", key='clear_resolved'):
                st.session_state.resolved_alerts = set()
                st.rerun()

        # ===================================================================
        # SYSTEM OVERVIEW
        # ===================================================================

        st.markdown("---")
        st.markdown("### üìä SYSTEM OVERVIEW")

        col1, col2, col3, col4, col5 = st.columns(5)
        
        risk_counts = predictions['risk'].value_counts()
        total_wards = len(predictions)
        
        col1.metric("Total Wards Monitored", total_wards)
        col2.metric("üü¢ Low Risk", risk_counts.get('Low', 0))
        col3.metric("üü° Moderate Risk", risk_counts.get('Moderate', 0))
        col4.metric("üî¥ High Risk", risk_counts.get('High', 0))
        col5.metric("Alert Load", f"{alert_load:.1%}")

        # ===================================================================
        # PREDICTION HISTORY TRACKING + TREND
        # ===================================================================

        current_date = datetime.now().date().isoformat()
        history_signature = (
            current_date,
            int(len(high_risk)),
            round(float(alert_load), 6),
            str(status_text),
        )

        history_data = load_prediction_history()
        if st.session_state.get('prediction_history_signature') != history_signature:
            history_data = append_prediction_history_entry(
                current_date=current_date,
                high_risk_count=len(high_risk),
                alert_load=alert_load,
                alert_status=status_text,
            )
            st.session_state['prediction_history_signature'] = history_signature

        st.markdown("---")
        st.markdown("### District Alert Trend (Historical)")
        if history_data:
            history_df = pd.DataFrame(history_data)
            if 'date' in history_df.columns:
                history_df['date'] = pd.to_datetime(history_df['date'], errors='coerce')
                history_df = history_df.dropna(subset=['date']).sort_values('date')
                history_df = history_df.set_index('date')

                if len(history_df) >= 2:
                    col_hist_1, col_hist_2 = st.columns(2)
                    with col_hist_1:
                        st.caption("High-risk wards over time")
                        st.line_chart(history_df['high_risk_wards'])
                    with col_hist_2:
                        st.caption("Alert load percentage over time")
                        st.line_chart(history_df['alert_load_pct'])
                elif len(history_df) == 1:
                    st.info("üìä Trend data will accumulate over multiple prediction cycles. Current history: 1 entry. Check back after next operational cycle.")
                    col_hist_1, col_hist_2 = st.columns(2)
                    with col_hist_1:
                        st.metric("Current High-Risk Wards", f"{int(history_df['high_risk_wards'].iloc[0])}")
                    with col_hist_2:
                        st.metric("Current Alert Load", f"{history_df['alert_load_pct'].iloc[0]:.1f}%")
                else:
                    st.info("No valid history entries available yet.")
            else:
                st.warning("History data format issue: missing 'date' column.")
        else:
            st.info("üìä No prediction history available yet. History tracking will begin after the first prediction run.")

        # ===================================================================
        # INTERACTIVE RISK HEATMAP - ALL WARDS RENDERED
        # ===================================================================

        st.markdown("---")
        st.markdown("### üó∫Ô∏è RISK HEATMAP - ALL WARDS")
        
        # Spatial visualization toggles
        spatial_col1, spatial_col2, spatial_col3 = st.columns([1, 1, 2])
        with spatial_col1:
            show_influence_arrows = st.checkbox(
                "üîó Show Spillover Arrows", 
                value=False,
                help="Draw arrows from high-risk peripheral wards to their neighbors"
            )
        with spatial_col2:
            show_neighbor_overlay = st.checkbox(
                "üî• Show Neighbor Risk", 
                value=False,
                help="Highlight wards surrounded by high-risk neighbors"
            )
        
        # Peripheral risk indicator
        with spatial_col3:
            peripheral_indicator = compute_peripheral_indicator(predictions)
            if peripheral_indicator and peripheral_indicator.get('available'):
                if peripheral_indicator['high_risk_count'] > 0:
                    st.metric(
                        "Peripheral High-Risk Wards",
                        f"{peripheral_indicator['percentage']:.0f}%",
                        delta=f"{peripheral_indicator['peripheral_high_count']}/{peripheral_indicator['high_risk_count']} wards",
                        delta_color="off"
                    )
                else:
                    st.metric("Peripheral High-Risk Wards", "N/A", delta="No high-risk wards")
        
        # Simulation toggle (separate row)
        sim_col1, sim_col2 = st.columns([1, 3])
        with sim_col1:
            enable_spread_simulation = st.checkbox(
                "üß™ Enable Spread Simulation",
                value=False,
                help="Synthetic spatial diffusion simulation (visualization only, does not alter model predictions)"
            )
        
        # Simulation parameters and execution
        sim_df = None
        sim_summary = None
        if enable_spread_simulation:
            with sim_col2:
                sim_alpha = st.slider(
                    "Diffusion coefficient (Œ±)",
                    min_value=0.10,
                    max_value=0.15,
                    value=0.12,
                    step=0.01,
                    help="Higher Œ± = more spread influence from peripheral wards"
                )
            
            # Run simulation
            sim_df = simulate_spatial_spread(predictions, threshold=threshold, alpha=sim_alpha)
            if sim_df is not None:
                sim_summary = get_spread_simulation_summary(sim_df)
            
            # Display simulation banner
            st.warning(
                f"üß™ **SYNTHETIC SPATIAL DIFFUSION SIMULATION ENABLED** (Œ± = {sim_alpha:.2f})  \n"
                "This visualization shows hypothetical spread patterns. "
                "**Core model predictions remain unchanged.**"
            )
        
        moderate_cutoff = float(threshold) * float(MODERATE_RISK_RATIO)
        st.markdown(
            f"**Legend:** üü¢ Low (<{moderate_cutoff:.3f}) | "
            f"üü° Moderate ({moderate_cutoff:.3f}-{threshold:.3f}) | "
            f"üî¥ High (‚â•{threshold:.3f}) | ‚ö™ No Data"
        )
        
        # Spatial overlay legend (if enabled)
        if show_influence_arrows or show_neighbor_overlay:
            legend_parts = []
            if show_influence_arrows:
                legend_parts.append("üü† Dashed lines = spillover risk from peripheral wards")
            if show_neighbor_overlay:
                legend_parts.append("üî¥ Circles = wards with high-risk neighbors (size = intensity)")
            st.caption(" | ".join(legend_parts))

        try:
            gdf = gpd.read_file(os.path.join(os.path.dirname(__file__), 'geo', 'coimbatore.geojson'))
            
            # Use simulated data if simulation enabled, else use original predictions
            if enable_spread_simulation and sim_df is not None:
                # Create a view DataFrame for rendering with simulated probabilities
                render_df = sim_df.copy()
                render_df['probability'] = render_df['simulated_probability']
                render_df['risk'] = render_df['simulated_risk']
            else:
                render_df = predictions
            
            heatmap = create_heatmap_with_all_wards(
                render_df, 
                gdf, 
                threshold,
                show_influence_arrows=show_influence_arrows,
                show_neighbor_overlay=show_neighbor_overlay
            )
            st_folium(heatmap, width=1400, height=600)
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Could not render heatmap: {str(e)}")
        
        st.caption("üìå **How to read this map:** Display bands (green/yellow/red) show risk zones for interpretability. Decision threshold (‚ö´ black line) triggers alerts. Colors beyond threshold indicate zones requiring immediate attention.")

        # ===================================================================
        # SIMULATION RESULTS PANEL (when simulation enabled)
        # ===================================================================
        
        if enable_spread_simulation and sim_summary and sim_summary.get('available'):
            with st.expander("üß™ Simulation Results", expanded=True):
                st.info(
                    "üìä **Spatial spread shown here is SYNTHETIC SIMULATION for demonstration purposes only.** "
                    "It does not alter core model predictions, thresholds, or risk classifications."
                )
                
                sim_col1, sim_col2, sim_col3, sim_col4 = st.columns(4)
                with sim_col1:
                    st.metric("Affected Wards", sim_summary['affected_wards_count'])
                with sim_col2:
                    st.metric("New High-Risk (Simulated)", sim_summary['new_high_risk_count'])
                with sim_col3:
                    st.metric("Max Spread Delta", f"+{sim_summary['max_spread_delta']:.1%}")
                with sim_col4:
                    st.metric("Avg Spread Delta", f"+{sim_summary['avg_spread_delta']:.1%}")
                
                if sim_summary['spread_sources']:
                    st.markdown(f"**Spread Sources:** {', '.join(sorted(sim_summary['spread_sources']))}")
                
                # Show comparison table
                if sim_df is not None:
                    affected = sim_df[sim_df['spread_delta'] > 0.001].sort_values('spread_delta', ascending=False)
                    if len(affected) > 0:
                        st.markdown("**Top Affected Wards:**")
                        affected_display = affected[['ward_id', 'probability', 'simulated_probability', 'spread_delta', 'risk', 'simulated_risk']].head(10).copy()
                        affected_display = affected_display.rename(columns={
                            'ward_id': 'Ward',
                            'probability': 'Original P',
                            'simulated_probability': 'Simulated P',
                            'spread_delta': 'Delta',
                            'risk': 'Original Risk',
                            'simulated_risk': 'Simulated Risk'
                        })
                        affected_display['Original P'] = affected_display['Original P'].apply(lambda x: f"{x:.1%}")
                        affected_display['Simulated P'] = affected_display['Simulated P'].apply(lambda x: f"{x:.1%}")
                        affected_display['Delta'] = affected_display['Delta'].apply(lambda x: f"+{x:.1%}")
                        st.dataframe(affected_display, use_container_width=True, hide_index=True)

        # ===================================================================
        # SPATIAL SPREAD ANALYSIS (Optional Expander)
        # ===================================================================
        
        if show_influence_arrows or show_neighbor_overlay:
            with st.expander("üìä Spatial Spread Analysis Details", expanded=False):
                spatial_summary = get_spatial_viz_summary(predictions)
                
                if spatial_summary['all_available']:
                    col_sp1, col_sp2, col_sp3 = st.columns(3)
                    
                    # Peripheral indicator details
                    if peripheral_indicator and peripheral_indicator.get('available'):
                        with col_sp1:
                            st.markdown("**Peripheral Ward Analysis**")
                            st.write(f"‚Ä¢ {peripheral_indicator['peripheral_high_count']} peripheral wards are high-risk")
                            st.write(f"‚Ä¢ {peripheral_indicator['high_risk_count']} total high-risk wards")
                            st.write(f"‚Ä¢ **{peripheral_indicator['percentage']:.1f}%** are on city boundary")
                    
                    # Influence arrows summary
                    if show_influence_arrows:
                        arrows = compute_infection_influence_arrows(predictions, threshold=threshold)
                        with col_sp2:
                            st.markdown("**Spillover Risk Connections**")
                            if arrows:
                                unique_sources = len(set(a['from_ward'] for a in arrows))
                                unique_targets = len(set(a['to_ward'] for a in arrows))
                                st.write(f"‚Ä¢ {len(arrows)} spillover connections")
                                st.write(f"‚Ä¢ {unique_sources} source wards (high-risk peripheral)")
                                st.write(f"‚Ä¢ {unique_targets} potentially affected neighbors")
                            else:
                                st.write("No spillover connections detected")
                    
                    # Neighbor risk summary
                    if show_neighbor_overlay:
                        neighbor_data = compute_neighbor_risk_overlay(predictions)
                        with col_sp3:
                            st.markdown("**Neighbor Risk Clustering**")
                            if neighbor_data:
                                high_intensity = [w for w, d in neighbor_data.items() if d['intensity'] > 0.3]
                                very_high = [w for w, d in neighbor_data.items() if d['high_risk_neighbor_count'] >= 3]
                                st.write(f"‚Ä¢ {len(high_intensity)} wards with elevated neighbor risk")
                                st.write(f"‚Ä¢ {len(very_high)} wards surrounded by 3+ high-risk neighbors")
                            else:
                                st.write("No neighbor clustering detected")
                else:
                    st.info("Spatial metadata not fully available. Some visualizations may be limited.")


        # ===================================================================
        # RISK ASSESSMENT TABLE
        # ===================================================================

        st.markdown("---")
        st.markdown("### üìã DETAILED RISK ASSESSMENT TABLE")

        table_df = predictions.copy()
        table_df['Zone'] = table_df['ward_id'].apply(map_ward_to_zone)
        table_df['Probability %'] = (table_df['probability'] * 100).round(1).astype(str) + '%'
        
        display_cols = ['ward_id', 'Zone', 'probability', 'Probability %', 'risk']
        table_df = table_df[display_cols].sort_values('probability', ascending=False)
        table_df = table_df.rename(columns={
            'ward_id': 'Ward',
            'probability': 'Outbreak Probability',
            'risk': 'Risk Level'
        })

        st.dataframe(
            table_df,
            use_container_width=True,
            hide_index=True,
            column_config={
                'Ward': st.column_config.TextColumn('Ward', width=80),
                'Zone': st.column_config.TextColumn('Zone', width=100),
                'Outbreak Probability': st.column_config.ProgressColumn(
                    'Outbreak Probability',
                    min_value=0.0,
                    max_value=1.0,
                    format='%.3f',
                    width=150
                ),
                'Probability %': st.column_config.TextColumn('Probability %', width=100),
                'Risk Level': st.column_config.TextColumn('Risk Level', width=100),
            }
        )

        # ===================================================================
        # OPERATIONAL METADATA CARD
        # ===================================================================

        st.markdown("---")
        st.markdown("### üìã OPERATIONAL METADATA")

        metadata = model_metadata
        cv_metrics = metadata.get('cv_metrics', {})
        global_threshold = metadata.get('global_threshold')
        global_threshold_text = f"{float(global_threshold):.3f}" if global_threshold is not None else 'N/A'

        col1, col2, col3, col4 = st.columns(4)

        col1.metric("CV Recall", f"{cv_metrics.get('recall_mean', 0):.2%} ¬± {cv_metrics.get('recall_std', 0):.2%}")
        col2.metric("CV F1-Score", f"{cv_metrics.get('f1_mean', 0):.1%}")
        col3.metric("CV ROC-AUC", f"{cv_metrics.get('roc_auc_mean', 0):.4f} ¬± {cv_metrics.get('roc_auc_std', 0):.4f}")
        calibration_method = str(metadata.get('calibration', {}).get('method', 'N/A')).title()
        calibration_source = str(metadata.get('calibration_source', metadata.get('calibration', {}).get('fitted_on', 'N/A'))).upper()
        col4.metric("Calibration", f"{calibration_method} ({calibration_source})")

        col1, col2, col3, col4 = st.columns(4)

        col1.write(f"**Artifact Version:** {metadata.get('artifact_version', 'N/A')}")
        col2.write(f"**Global Threshold:** {global_threshold_text}")
        col3.write(f"**CV Folds:** {metadata.get('n_folds', 'N/A')}")
        col4.write(f"**Features:** {metadata.get('n_features', 'N/A')}")

        training_range = metadata.get('training_data_range', {})
        col1, col2 = st.columns(2)
        col1.write(f"**Training Period:** {training_range.get('start', 'N/A')} to {training_range.get('end', 'N/A')}")
        col2.write(f"**Training Date (UTC):** {metadata.get('trained_at_utc', 'N/A')}")
        st.caption(
            f"Artifact {metadata.get('artifact_version', 'N/A')} trained on full dataset "
            f"({metadata.get('n_samples', 'N/A')} samples); alerts generated without retraining."
        )

        # ===================================================================
        # DATA QUALITY CARD
        # ===================================================================

        st.markdown("---")
        st.markdown("### üìä DATA QUALITY")

        col1, col2, col3, col4 = st.columns(4)

        col1.metric("Total Rows", len(data))
        col2.metric("Ward Coverage", f"{data['ward_id'].nunique()} wards")
        
        missing_pct = (data.isnull().sum().sum() / (len(data) * len(data.columns)) * 100) if len(data) > 0 else 0
        col3.metric("Missing Values", f"{missing_pct:.2f}%")
        
        if 'week_start_date' in data.columns:
            try:
                latest_date = pd.to_datetime(data['week_start_date']).max()
                col4.write(f"**Latest Timestamp:** {latest_date.strftime('%Y-%m-%d')}")
            except (ValueError, TypeError, AttributeError):
                col4.write("**Latest Timestamp:** N/A")

        # ===================================================================
        # ADVANCED DIAGNOSTICS (EXPANDER)
        # ===================================================================

        with st.expander("üìà Advanced Diagnostics"):
            st.markdown("#### Model Performance Metrics (Per Fold)")
            fold_metrics = metadata.get('cv_fold_metrics', [])
            if fold_metrics:
                fold_data = []
                for fold in fold_metrics:
                    if not fold.get('skipped'):
                        fold_data.append({
                            'Fold': fold['fold_id'],
                            'Recall': f"{fold['recall']:.1%}",
                            'Precision': f"{fold['precision']:.1%}",
                            'F1': f"{fold['f1_score']:.1%}",
                            'ROC-AUC': f"{fold['roc_auc']:.4f}",
                        })
                if fold_data:
                    st.dataframe(fold_data, use_container_width=True, hide_index=True)

            st.markdown("#### Model Configuration")
            st.json({
                'evaluation_method': metadata.get('evaluation_method', 'N/A'),
                'calibration_source': metadata.get('calibration_source', 'N/A'),
                'final_model_full_data': metadata.get('final_model_trained_on_full_data', False),
                'features_count': metadata.get('n_features', 'N/A'),
                'training_samples': metadata.get('n_samples', 'N/A'),
            })
        
        # ===================================================================
        # SYSTEM READINESS & DEPLOYMENT STATUS
        # ===================================================================
        
        display_system_readiness(metadata=metadata, data_path=data_path)

    # =========================================================================
    # ERROR STATE
    # =========================================================================

    else:
        if error_occurred:
            st.error(f"‚ö†Ô∏è Cannot enter {OPERATIONAL_MODE_LABEL}. Please check model and data availability.")
            st.markdown("""
            **Troubleshooting:**
            - Verify model exists at: `model/final_outbreak_model_v3.pkl`
            - Verify data file exists in `data/` directory
            - Check logs for detailed error messages
            """)
        else:
            st.warning("‚ö†Ô∏è System initialization failed.")


# =============================================================================
# PAGE: ADMIN - RETRAINING
# =============================================================================

def page_admin():
    """Admin page for retraining model"""
    st.title("‚öôÔ∏è Administration Panel")
    st.markdown("**Advanced Operations** ‚Äî Model retraining, data validation, diagnostics")

    st.warning("""
    ‚ö†Ô∏è **WARNING:** Retraining will:
    - Evaluate model on full historical dataset
    - Overwrite deployed artifact: `model/final_outbreak_model_v3.pkl` (`3.0-final`)  
    - Require 10-30 minutes to complete
    """)

    col1, col2 = st.columns([2, 1])

    with col1:
        data_path = st.text_input(
            "Data file path",
            value="data/coimbatore_unified_master_dataset_with_zone.csv",
            help="CSV file with historical outbreak data"
        )

    with col2:
        apply_noise = st.checkbox("Apply training noise", value=False)
        include_spatial = st.checkbox("Include spatial features", value=True, 
            help="Add neighbor lag + peripheral ward features (7 additional features)")

    # Determine model path based on spatial features
    if include_spatial:
        model_output_path = 'artifacts/outbreak_model_spatial_v1.pkl'
        st.info("üåê Spatial model selected ‚Äî will save to `artifacts/outbreak_model_spatial_v1.pkl`")
    else:
        model_output_path = 'model/final_outbreak_model_v3.pkl'

    if st.button("üöÄ Retrain Model on Full Dataset", type="primary", use_container_width=True):
        st.info("üîÑ Training started... This may take several minutes.")

        try:
            from model.train import OutbreakModelTrainer

            trainer = OutbreakModelTrainer(
                model_path=model_output_path,
                include_spatial_features=include_spatial
            )
            df = trainer.load_data(data_path)
            trainer.train(df, apply_realism_noise=apply_noise)

            st.success("‚úÖ Model trained successfully!" + (" (with spatial features)" if include_spatial else ""))
            
            # Display results
            if hasattr(trainer, 'metrics'):
                cv_metrics = trainer.metrics.get('cv_metrics', {})
                trained_threshold = trainer.metrics.get('global_threshold')
                trained_threshold_text = f"{float(trained_threshold):.3f}" if trained_threshold is not None else 'N/A'
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("CV Recall", f"{cv_metrics.get('recall_mean', 0):.1%}")
                col2.metric("CV F1", f"{cv_metrics.get('f1_mean', 0):.1%}")
                col3.metric("CV ROC-AUC", f"{cv_metrics.get('roc_auc_mean', 0):.3f}")
                col4.metric("Global Threshold", trained_threshold_text)

                # Clear cache so next run loads updated model
                load_predictor.clear()

        except Exception as e:
            st.error(f"‚ùå Training failed: {str(e)}")

    st.markdown("---")
    st.markdown("### üìã Artifact Deployment Status")

    try:
        predictor, error = load_predictor()
        if predictor:
            metadata = predictor.get_model_metadata()
            st.success(f"‚úÖ Artifact {metadata.get('artifact_version', 'N/A')} deployed and ready")
            
            cv_metrics = metadata.get('cv_metrics', {})
            threshold_value = metadata.get('global_threshold')
            threshold_text = f"{float(threshold_value):.3f}" if threshold_value is not None else 'N/A'
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("CV Recall", f"{cv_metrics.get('recall_mean', 0):.1%}")
            col2.metric("CV F1", f"{cv_metrics.get('f1_mean', 0):.1%}")
            col3.metric("CV ROC-AUC", f"{cv_metrics.get('roc_auc_mean', 0):.3f}")
            col4.metric("Threshold", threshold_text)
            
        else:
            st.error(f"‚ùå Model not found: {error}")
    except Exception as e:
        st.error(f"‚ùå Error loading model: {str(e)}")


# =============================================================================
# PAGE: DATA INTEGRATION
# =============================================================================

def display_data_quality_monitor(df: pd.DataFrame) -> dict:
    """
    Display data quality metrics before predictions
    Shows: missing values %, duplicates, ward coverage %, date alignment
    """
    if df is None or len(df) == 0:
        return {'status': 'error', 'message': 'No data available'}
    
    try:
        # Calculate metrics
        total_rows = len(df)
        duplicate_rows = len(df[df.duplicated()])
        missing_pct = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
        
        unique_wards = df['ward_id'].nunique() if 'ward_id' in df.columns else 0
        coverage_pct = (unique_wards / 100) * 100 if unique_wards > 0 else 0  # Assume ~100 wards in Coimbatore
        
        # Date alignment
        date_range = None
        if 'week_start_date' in df.columns:
            try:
                date_range = f"{df['week_start_date'].min()} to {df['week_start_date'].max()}"
            except (ValueError, TypeError, AttributeError):
                date_range = "Unknown"
        
        # Display in expander
        with st.expander("üìã **Data Quality Monitor** (Pre-prediction validation)", expanded=False):
            col1, col2, col3, col4 = st.columns(4)
            
            col1.metric(
                "Missing Values",
                f"{missing_pct:.1f}%",
                help="Percentage of null/missing values in dataset"
            )
            
            col2.metric(
                "Duplicate Rows",
                duplicate_rows,
                help="Number of exact duplicate records"
            )
            
            col3.metric(
                "Ward Coverage",
                f"{unique_wards} wards",
                help=f"~{coverage_pct:.0f}% of city covered"
            )
            
            col4.metric(
                "Date Range",
                "Valid" if date_range else "Unknown",
                help=f"{date_range}" if date_range else "Date fields not aligned"
            )
            
            # Detailed missing values per column
            if missing_pct > 0:
                st.markdown("#### Missing Values by Column:")
                missing_cols = df.isnull().sum()
                missing_cols = missing_cols[missing_cols > 0].sort_values(ascending=False)
                if len(missing_cols) > 0:
                    for col, count in missing_cols.items():
                        pct = (count / len(df)) * 100
                        st.caption(f"  ‚Ä¢ {col}: {count} rows ({pct:.1f}%)")
            else:
                st.success("‚úì No missing values detected")
            
            # Data alignment check
            st.markdown("#### Date Alignment Check:")
            if 'week_start_date' in df.columns:
                st.success(f"‚úì {date_range}")
            else:
                st.warning("‚ö† Date column not found")
        
        return {
            'status': 'success',
            'missing_pct': missing_pct,
            'duplicate_rows': duplicate_rows,
            'unique_wards': unique_wards,
            'date_range': date_range
        }
    except Exception as e:
        st.warning(f"Could not generate quality report: {str(e)}")
        return {'status': 'error', 'message': str(e)}


def display_system_readiness(metadata: dict | None = None, data_path: str | None = None):
    """
    Display system readiness status, deployment configuration, and architecture
    """
    with st.expander("‚öôÔ∏è **System Readiness & Deployment Status**", expanded=False):
        col1, col2 = st.columns([1.5, 1.5])
        
        with col1:
            st.markdown("#### üìÖ Model Management")
            metadata = metadata or {}
            training_range = metadata.get('training_data_range', {})
            threshold_value = metadata.get('global_threshold')
            threshold_text = f"{float(threshold_value):.3f}" if threshold_value is not None else 'N/A'
            st.write(f"‚Ä¢ **Artifact Version:** {metadata.get('artifact_version', 'N/A')}")
            st.write(f"‚Ä¢ **Training Date:** {metadata.get('trained_at_utc', 'N/A')}")
            st.write(f"‚Ä¢ **Retraining Frequency:** {metadata.get('retraining_frequency_days', 'N/A')} days")
            st.write(f"‚Ä¢ **Training Data Range:** {training_range.get('start', 'N/A')} to {training_range.get('end', 'N/A')}")
            st.write(f"‚Ä¢ **CV Folds:** {metadata.get('n_folds', 'N/A')} (TimeSeriesSplit)")
            st.write(f"‚Ä¢ **Global Threshold:** {threshold_text}")
        
        with col2:
            st.markdown("#### üîå Data Ingestion")
            st.write("‚Ä¢ **Health Data:** CSV + Ready for API")
            st.write("‚Ä¢ **Water Quality:** CSV + Ready for API")
            st.write("‚Ä¢ **Rainfall Data:** CSV + Ready for API")
            st.write("‚Ä¢ **Integration:** Weekly merge on (ward_id, week_start_date)")
            st.write("‚Ä¢ **Update Frequency:** Weekly")
            if data_path:
                st.write(f"‚Ä¢ **Active Data File:** {data_path}")
        
        st.markdown("---")
        st.markdown("#### üó∫Ô∏è Ward Mapping & Coverage")
        
        with st.expander("View Ward Mapping Table", expanded=False):
            ward_zones = {
                'Ward_1-5': 'North Zone',
                'Ward_6-15': 'Central Zone',
                'Ward_16-25': 'South Zone',
                'Ward_26-35': 'East Zone',
                'Ward_36-50': 'West Zone',
                'Ward_51-100': 'Peripheral Wards'
            }
            
            mapping_df = pd.DataFrame([
                {'Ward Range': k, 'Zone': v} for k, v in ward_zones.items()
            ])
            st.dataframe(mapping_df, use_container_width=True, hide_index=True)
        
        st.markdown("---")
        threshold_value = (metadata or {}).get('global_threshold')
        threshold_text = f"{float(threshold_value):.3f}" if threshold_value is not None else 'N/A'
        st.markdown("#### üèóÔ∏è System Architecture")
        st.markdown(f"""
        ```
        DATA SOURCES (Multi-Source Integration)
        ‚îú‚îÄ‚îÄ üè• Health Surveillance (Hospital Cases)
        ‚îú‚îÄ‚îÄ üíß Water Quality (Turbidity, E.coli, pH)
        ‚îî‚îÄ‚îÄ üåßÔ∏è Rainfall (Weekly Precipitation)
                    ‚Üì
        INTEGRATION LAYER (Auto-merge on date + ward)
                    ‚Üì
        FEATURE ENGINEERING (27 engineered features)
        ‚îú‚îÄ‚îÄ Rolling averages (3-week)
        ‚îú‚îÄ‚îÄ Growth rates (contamination, rainfall, cases)
        ‚îú‚îÄ‚îÄ Interaction terms (rainfall √ó water quality)
        ‚îî‚îÄ‚îÄ Temporal indicators (monsoon, outbreak_last_week)
                    ‚Üì
        PREDICTION ENGINE (XGBoost, TimeSeriesSplit CV)
                    ‚Üì
        ALERT & INTERVENTION (Real-time risk assessment)
        ‚îú‚îÄ‚îÄ üö® Automated alerting (threshold = {threshold_text})
        ‚îú‚îÄ‚îÄ üéØ Contributing factor analysis
        ‚îú‚îÄ‚îÄ üíª Intervention simulation
        ‚îî‚îÄ‚îÄ üìä Outcome tracking
        ```
        """)

        st.caption(f"Operational forecast horizon: {FORECAST_HORIZON_DAYS} days")
        
        st.markdown("#### ‚úÖ Deployment Readiness Checklist")
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("‚úÖ Model trained & persisted")
            st.write("‚úÖ Auto-prediction enabled")
            st.write("‚úÖ Auto-alerting enabled")
            st.write("‚úÖ Multi-source data integration")
        
        with col2:
            st.write("‚úÖ Data quality monitoring")
            st.write("‚úÖ Intervention simulation")
            st.write("‚úÖ Temporal analysis enabled")
            st.write("‚úÖ No manual retraining needed for daily ops")


def page_temporal_monitoring():
    """
    Temporal monitoring view: select ward and view 6-week trends
    """
    st.title("üìà Temporal Monitoring View")
    st.markdown("**Ward Trend Analysis** ‚Äî View historical patterns and predicted risk")
    
    # Load data
    data_path = resolve_default_data_path()
    data, load_error = load_data(data_path)
    
    if load_error or data is None:
        st.error(f"Cannot load data: {load_error}")
        return
    
    st.markdown("---")
    
    # Ward selector
    ward_list = sorted(data['ward_id'].unique())
    selected_ward = st.selectbox(
        "Select Ward",
        ward_list,
        help="Choose a ward to view historical trends"
    )
    
    # Load predictor
    predictor, pred_error = load_predictor()
    if pred_error or predictor is None:
        st.error("Cannot load model")
        return

    metadata = predictor.get_model_metadata()
    threshold_value = metadata.get('global_threshold')
    if threshold_value is None:
        st.error("Cannot render temporal monitoring: missing global_threshold in model metadata")
        return
    threshold = float(threshold_value)

    # Get ward data (past 6 weeks)
    ward_data = data[data['ward_id'] == selected_ward].tail(6).sort_values('week_start_date')

    cases_col = 'reported_cases' if 'reported_cases' in ward_data.columns else 'cases'
    ecoli_col = 'ecoli_index' if 'ecoli_index' in ward_data.columns else 'e_coli'
    
    if len(ward_data) == 0:
        st.warning(f"No data available for {selected_ward}")
        return
    
    zone = map_ward_to_zone(selected_ward)
    st.subheader(f"{selected_ward} ({zone}) ‚Äî Past 6 Weeks + Prediction")
    
    # Create tabs for different views
    tab1, tab2, tab3 = st.tabs(["üìä Trends", "üéØ Risk Prediction", "üìã Raw Data"])
    
    with tab1:
        # Multi-line chart: cases, rainfall, contamination
        col1, col2 = st.columns(2)
        
        with col1:
            # Cases trend
            fig_cases = go.Figure()
            fig_cases.add_trace(go.Scatter(
                x=ward_data['week_start_date'],
                y=ward_data[cases_col],
                mode='lines+markers',
                name='Disease Cases',
                line=dict(color='red', width=2),
                marker=dict(size=8)
            ))
            fig_cases.update_layout(
                title="Disease Cases (Past 6 Weeks)",
                xaxis_title="Week",
                yaxis_title="Cases",
                hovermode='x unified',
                height=300
            )
            st.plotly_chart(fig_cases, use_container_width=True)
        
        with col2:
            # Rainfall trend
            fig_rain = go.Figure()
            fig_rain.add_trace(go.Scatter(
                x=ward_data['week_start_date'],
                y=ward_data['rainfall_mm'],
                mode='lines+markers',
                name='Rainfall (mm)',
                line=dict(color='blue', width=2),
                marker=dict(size=8)
            ))
            fig_rain.update_layout(
                title="Rainfall (Past 6 Weeks)",
                xaxis_title="Week",
                yaxis_title="Rainfall (mm)",
                hovermode='x unified',
                height=300
            )
            st.plotly_chart(fig_rain, use_container_width=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Contamination trend
            fig_contamination = go.Figure()
            if 'turbidity' in ward_data.columns and ecoli_col in ward_data.columns:
                fig_contamination.add_trace(go.Scatter(
                    x=ward_data['week_start_date'],
                    y=ward_data['turbidity'],
                    mode='lines+markers',
                    name='Turbidity (NTU)',
                    line=dict(color='orange', width=2),
                    yaxis='y1'
                ))
                fig_contamination.add_trace(go.Scatter(
                    x=ward_data['week_start_date'],
                    y=ward_data[ecoli_col],
                    mode='lines+markers',
                    name='E. coli',
                    line=dict(color='purple', width=2),
                    yaxis='y2'
                ))
                fig_contamination.update_layout(
                    title="Water Contamination (Dual Axis)",
                    xaxis_title="Week",
                    yaxis=dict(title="Turbidity (NTU)"),
                    yaxis2=dict(title="E. coli Index", overlaying='y', side='right'),
                    hovermode='x unified',
                    height=300
                )
            st.plotly_chart(fig_contamination, use_container_width=True)
        
        with col2:
            # Trend summary
            st.markdown("#### üìä Trend Summary")
            
            if len(ward_data) > 0:
                latest = ward_data.iloc[-1]
                prev = ward_data.iloc[-2] if len(ward_data) > 1 else latest
                
                cases_change = latest[cases_col] - prev[cases_col]
                rainfall_change = latest['rainfall_mm'] - prev['rainfall_mm']
                
                st.metric(
                    "Cases (Week-on-Week)",
                    f"{int(latest[cases_col])}",
                    f"{int(cases_change):+d}",
                    delta_color="inverse"
                )
                
                st.metric(
                    "Rainfall This Week",
                    f"{latest['rainfall_mm']:.1f} mm",
                    f"{rainfall_change:+.1f} mm"
                )
                
                if 'turbidity' in ward_data.columns:
                    st.metric(
                        "Current Turbidity",
                        f"{latest['turbidity']:.1f} NTU"
                    )
    
    with tab2:
        # Get predicted risk for this ward
        st.markdown("#### üéØ Next Week Outbreak Risk Prediction")
        
        try:
            predictions = predictor.predict_latest_week(df=data[data['ward_id'] == selected_ward])
            
            if predictions is not None and len(predictions) > 0:
                pred = predictions.iloc[0]
                
                col1, col2, col3 = st.columns(3)
                
                col1.metric(
                    "Outbreak Probability",
                    f"{pred['probability']:.1%}",
                    help="Predicted probability of outbreak next week"
                )
                
                col2.metric(
                    "Risk Level",
                    pred['risk'],
                    help="Classification: Low/Moderate/High"
                )
                
                col3.metric(
                    "Decision Threshold",
                    f"{threshold:.3f}",
                    help="Alert triggered if probability ‚â• threshold"
                )
                
                # If high risk, show contributing factors
                if pred['probability'] > threshold:
                    st.warning(f"‚ö†Ô∏è **{selected_ward} above alert threshold**")
                    
                    contributing = get_contributing_factors(predictor, ward_data.iloc[-1:])
                    if contributing:
                        st.markdown("#### Top Contributing Factors:")
                        for i, factor in enumerate(contributing, 1):
                            st.write(f"{i}. **{factor['feature']}** (Value: {factor['value']:.1f})")
                else:
                    st.success(f"‚úÖ **{selected_ward} below alert threshold**")
            else:
                st.warning("No prediction available")
        
        except Exception as e:
            st.error(f"Prediction failed: {str(e)}")
    
    with tab3:
        st.markdown("#### üìã Raw Data (Past 6 Weeks)")
        
        display_cols = ['week_start_date', 'ward_id', 'zone', cases_col, 'rainfall_mm', 'turbidity', ecoli_col]
        display_cols = [col for col in display_cols if col in ward_data.columns]
        
        st.dataframe(
            ward_data[display_cols],
            use_container_width=True,
            hide_index=True
        )


def page_model_intelligence():
    """
    Model Intelligence page: CV metrics, explainability, feature importance
    """
    st.title("üß† Model Intelligence")
    st.markdown("**Model Performance & Explainability** ‚Äî CV metrics, feature importance, calibration")
    
    # Load model metadata
    predictor, error = load_predictor()
    if error or predictor is None:
        st.error("Cannot load model")
        return
    
    metadata = predictor.get_model_metadata()
    
    st.markdown("---")

    cv_metrics = metadata.get('cv_metrics', {})
    training_range = metadata.get('training_data_range', {})
    calibration = metadata.get('calibration', {})
    calibration_method = str(calibration.get('method', 'N/A')).title()
    calibration_source = str(metadata.get('calibration_source', calibration.get('fitted_on', 'N/A'))).upper()

    st.markdown("#### Production Snapshot")
    threshold_value = metadata.get('global_threshold')
    threshold_text = f"{float(threshold_value):.2f}" if threshold_value is not None else 'N/A'
    col1, col2, col3 = st.columns(3)
    col1.metric("Artifact Version", str(metadata.get('artifact_version', 'N/A')))
    col2.metric("Global Threshold", threshold_text)
    col3.metric("Calibration", f"{calibration_method} ({calibration_source})")

    col1, col2, col3 = st.columns(3)
    col1.metric("CV Recall", f"{cv_metrics.get('recall_mean', 0):.2%} ¬± {cv_metrics.get('recall_std', 0):.2%}")
    col2.metric("CV ROC-AUC", f"{cv_metrics.get('roc_auc_mean', 0):.4f} ¬± {cv_metrics.get('roc_auc_std', 0):.4f}")
    col3.metric("Training Date", str(metadata.get('trained_at_utc', 'N/A')))

    st.caption(f"Training Range: {training_range.get('start', 'N/A')} to {training_range.get('end', 'N/A')}")

    tab1, tab2, tab3, tab4 = st.tabs(["üìä CV Metrics", "üéØ Features", "üìà Calibration", "‚öôÔ∏è Configuration"])

    with tab1:
        st.markdown("#### Cross-Validation Performance")
        fold_metrics = cv_metrics.get('fold_metrics', metadata.get('cv_fold_metrics', []))
        if fold_metrics:
            fold_data = []
            for fold in fold_metrics:
                if fold.get('skipped'):
                    continue
                fold_id = fold.get('fold', fold.get('fold_id'))
                fold_data.append({
                    'Fold': fold_id,
                    'Recall': f"{fold.get('recall', 0):.1%}",
                    'Precision': f"{fold.get('precision', 0):.1%}",
                    'F1': f"{fold.get('f1', fold.get('f1_score', 0)):.1%}",
                    'ROC-AUC': f"{fold.get('roc_auc', 0):.4f}",
                    'Threshold': f"{fold.get('threshold', fold.get('best_threshold', threshold_value if threshold_value is not None else float('nan'))):.2f}",
                })
            if fold_data:
                st.dataframe(fold_data, use_container_width=True, hide_index=True)
        st.info("Thresholds are tuned only on validation folds; final global threshold is fold-mean for operational use.")

    with tab2:
        st.markdown("#### Active Feature Columns")
        feature_columns = predictor.feature_columns if predictor.feature_columns is not None else []
        st.write(f"Total features: {len(feature_columns)}")
        if feature_columns:
            st.dataframe(pd.DataFrame({'feature': feature_columns}), use_container_width=True, hide_index=True)

    with tab3:
        st.markdown("#### Calibration Details")
        st.write(f"**Method:** {calibration_method}")
        st.write(f"**Source:** {calibration_source}")
        threshold_text_cal = f"{float(threshold_value):.3f}" if threshold_value is not None else 'N/A'
        st.write(f"**Decision Threshold:** {threshold_text_cal}")

    with tab4:
        st.markdown("#### Model Configuration")
        config = {
            'Algorithm': 'XGBoost Binary Classifier',
            'Evaluation': f"TimeSeriesSplit CV ({metadata.get('n_folds', 'N/A')} folds)",
            'Training Samples': metadata.get('n_samples', 'N/A'),
            'Features': metadata.get('n_features', 'N/A'),
            'Training Date': metadata.get('trained_at_utc', 'N/A'),
            'Training Range': f"{training_range.get('start', 'N/A')} to {training_range.get('end', 'N/A')}",
            'Best Hyperparameters': metadata.get('best_params', {}),
        }
        st.json(config)


def page_environmental_analysis():
    """
    Environmental Analysis page: correlation heatmap, seasonal patterns
    """
    st.title("üî¨ Environmental Analysis")
    st.markdown("**Correlation & Seasonal Patterns** ‚Äî Understand data relationships and temporal trends")
    
    # Load data
    data_path = resolve_default_data_path()
    data, load_error = load_data(data_path)
    
    if load_error or data is None:
        st.error(f"Cannot load data: {load_error}")
        return
    
    st.markdown("---")
    
    tab1, tab2, tab3 = st.tabs(["üîó Correlations", "üìÖ Seasonal Patterns", "üéØ Ward Comparisons"])

    cases_col = 'reported_cases' if 'reported_cases' in data.columns else ('cases' if 'cases' in data.columns else None)
    ecoli_col = 'ecoli_index' if 'ecoli_index' in data.columns else ('e_coli' if 'e_coli' in data.columns else None)
    
    with tab1:
        st.markdown("#### Feature Correlations with Disease Cases")
        
        # Calculate correlations
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        
        # Focus on key features
        key_features = [cases_col, 'rainfall_mm', 'turbidity', ecoli_col, 'temperature', 'ph']
        key_features = [feature for feature in key_features if feature is not None]
        available_features = [col for col in key_features if col in numeric_cols]
        
        if len(available_features) > 1:
            corr_matrix = data[available_features].corr()
            
            # Create heatmap
            fig_heatmap = go.Figure(data=go.Heatmap(
                z=corr_matrix.values,
                x=corr_matrix.columns,
                y=corr_matrix.columns,
                colorscale='RdBu',
                zmid=0,
                text=np.round(corr_matrix.values, 2),
                texttemplate='%{text:.2f}',
                textfont={"size": 10}
            ))
            
            fig_heatmap.update_layout(
                title="Correlation Matrix: Environmental Factors vs Disease",
                height=400,
                width=600
            )
            
            st.plotly_chart(fig_heatmap, use_container_width=True)
            
            # Correlation analysis basis
            corr_sample_size = len(data)
            date_range_start = pd.to_datetime(data['week_start_date']).min() if 'week_start_date' in data.columns else None
            date_range_end = pd.to_datetime(data['week_start_date']).max() if 'week_start_date' in data.columns else None
            date_range_text = f"{date_range_start.strftime('%Y-%m-%d')} to {date_range_end.strftime('%Y-%m-%d')}" if date_range_start and date_range_end else "Unknown"
            
            st.markdown("#### Correlation Analysis Basis:")
            col1, col2, col3 = st.columns(3)
            col1.metric("Sample Size", f"{corr_sample_size:,} week-ward observations")
            col2.metric("Time Period", date_range_text if len(date_range_text) < 25 else f"{corr_sample_size} records")
            col3.metric("Method", "Pearson Correlation")
            
            st.caption(f"Correlation computed across {corr_sample_size:,} integrated surveillance records covering {date_range_text}. Each observation represents weekly aggregated metrics per ward. Positive correlations indicate factors that rise together with disease cases; negative correlations indicate inverse relationships.")
            
            # Interpretation
            st.markdown("#### Key Correlations:")
            
            case_corr = corr_matrix[cases_col].sort_values(ascending=False) if cases_col in corr_matrix.columns else pd.Series(dtype=float)
            for feat, corr_val in case_corr.items():
                if feat != cases_col:
                    direction = "üî¥ positive" if corr_val > 0 else "üü¢ negative"
                    st.write(f"‚Ä¢ **{feat}** {direction} correlation: {corr_val:.3f}")
        else:
            st.warning("Insufficient numeric features for correlation analysis")
    
    with tab2:
        st.markdown("#### Seasonal Disease Patterns")
        
        # Monthly aggregation
        data_copy = data.copy()
        if 'week_start_date' in data_copy.columns:
            data_copy['month'] = pd.to_datetime(data_copy['week_start_date']).dt.month
            data_copy['month_name'] = pd.to_datetime(data_copy['week_start_date']).dt.strftime('%B')
            if cases_col is None:
                st.warning("Cases column not available for seasonal analysis")
            else:
                # Group by month and sort chronologically
                monthly_cases = data_copy.groupby(['month', 'month_name'])[cases_col].mean().reset_index()
                monthly_cases = monthly_cases.sort_values('month')
                
                monthly_rainfall = data_copy.groupby(['month', 'month_name'])['rainfall_mm'].mean().reset_index()
                monthly_rainfall = monthly_rainfall.sort_values('month')
            
                col1, col2 = st.columns(2)

                with col1:
                    # Monthly cases
                    fig_monthly_cases = go.Figure()
                    fig_monthly_cases.add_trace(go.Bar(
                        x=monthly_cases['month_name'],
                        y=monthly_cases[cases_col],
                        name='Avg Cases',
                        marker=dict(color='red')
                    ))
                    fig_monthly_cases.update_layout(
                        title="Average Cases by Month",
                        xaxis_title="Month",
                        yaxis_title="Cases",
                        height=300,
                        xaxis=dict(categoryorder='array', categoryarray=monthly_cases['month_name'].tolist())
                    )
                    st.plotly_chart(fig_monthly_cases, use_container_width=True)

                with col2:
                    # Monthly rainfall
                    fig_monthly_rain = go.Figure()
                    fig_monthly_rain.add_trace(go.Bar(
                        x=monthly_rainfall['month_name'],
                        y=monthly_rainfall['rainfall_mm'],
                        name='Avg Rainfall',
                        marker=dict(color='blue')
                    ))
                    fig_monthly_rain.update_layout(
                        title="Average Rainfall by Month",
                        xaxis_title="Month",
                        yaxis_title="Rainfall (mm)",
                        height=300,
                        xaxis=dict(categoryorder='array', categoryarray=monthly_rainfall['month_name'].tolist())
                    )
                    st.plotly_chart(fig_monthly_rain, use_container_width=True)

                # Seasonal insight
                st.info("""
                üåßÔ∏è **Monsoon Effect (June-October):**
                - Increased rainfall ‚Üí ‚Üë Cases
                - Monsoon seasons show 58% higher disease cases

                ‚òÄÔ∏è **Dry Season (November-May):**
                - Lower rainfall ‚Üí ‚Üì Cases
                - Better water quality sustainability
                """)
    
    with tab3:
        st.markdown("#### Ward-wise Comparison")
        
        # Zone-level summary
        if 'zone' in data.columns:
            agg_map = {
                'rainfall_mm': 'mean',
                'turbidity': 'mean',
            }
            if cases_col and cases_col in data.columns:
                agg_map[cases_col] = 'mean'
            if ecoli_col and ecoli_col in data.columns:
                agg_map[ecoli_col] = 'mean'

            zone_stats = data.groupby('zone').agg(agg_map).round(1)
            
            st.markdown("**Average Metrics by Zone:**")
            st.dataframe(zone_stats, use_container_width=True)
            
            # Heatmap: zones vs metrics
            fig_zone_heatmap = go.Figure(data=go.Heatmap(
                z=zone_stats.values,
                x=zone_stats.columns,
                y=zone_stats.index,
                colorscale='Viridis'
            ))
            
            fig_zone_heatmap.update_layout(
                title="Zone Comparison: Environmental Metrics",
                height=300
            )
            
            st.plotly_chart(fig_zone_heatmap, use_container_width=True)


def page_data_integration():
    """Data sources integration and validation page"""
    st.title("üìä Data Integration Module")
    st.markdown("""
    **Multi-Source Data Integration** ‚Äî Consolidate health surveillance, water quality, and rainfall data  
    Addresses: Lack of integration between water quality data and health surveillance systems
    """)
    
    st.markdown("---")
    
    # Load and display default integrated dataset
    st.subheader("üìä Current Integrated Dataset")
    try:
        data_path = resolve_default_data_path()
        integrated_ds, error = load_data(data_path)
        if integrated_ds is not None and len(integrated_ds) > 0:
            col1, col2, col3 = st.columns(3)
            col1.metric("üìä Total Rows", f"{len(integrated_ds):,}")
            col2.metric("üìç Unique Wards", f"{integrated_ds['ward_id'].nunique()}")
            col3.metric("üìÖ Weeks Covered", f"{integrated_ds['week_start_date'].nunique() if 'week_start_date' in integrated_ds.columns else 'N/A'}")
            
            st.markdown("#### Preview (First 20 Rows)")
            display_cols = ['week_start_date', 'ward_id', 'reported_cases', 'turbidity', 'ecoli_index', 'rainfall_mm']
            display_cols = [col for col in display_cols if col in integrated_ds.columns]
            st.dataframe(
                integrated_ds[display_cols].head(20),
                use_container_width=True,
                hide_index=True
            )
        else:
            st.warning("Could not load integrated dataset.")
    except Exception as e:
        st.warning(f"Could not load integrated dataset: {str(e)}")
    
    st.markdown("---")
    st.subheader("üì• Data Source Configuration")
    
    # Three columns for data inputs
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### üè• Health Surveillance Data")
        st.markdown("*Hospital reports, disease cases, fatalities*")
        health_source = st.selectbox(
            "Health data source",
            ["Auto-load from CSV (‚Üí Live source)", "Manual CSV upload"],
            key="health_source"
        )
        
        if health_source == "Auto-load from CSV (‚Üí Live source)":
            st.caption("‚úì Connected to data pipeline")
            st.button("üîÑ Load Health Data", key="load_health")
        else:
            st.file_uploader("Upload health data CSV", type=['csv'], key="health_upload")
    
    with col2:
        st.markdown("### üíß Water Quality Data")
        st.markdown("*Turbidity, E.coli, pH, contamination*")
        water_source = st.selectbox(
            "Water data source",
            ["Auto-load from CSV (‚Üí Live source)", "Manual CSV upload"],
            key="water_source"
        )
        
        if water_source == "Auto-load from CSV (‚Üí Live source)":
            st.caption("‚úì Connected to monitoring stations")
            st.button("üîÑ Load Water Data", key="load_water")
        else:
            st.file_uploader("Upload water data CSV", type=['csv'], key="water_upload")
    
    with col3:
        st.markdown("### üåß Rainfall Data")
        st.markdown("*Precipitation, monsoon patterns*")
        rain_source = st.selectbox(
            "Rainfall data source",
            ["Auto-load from CSV (‚Üí Live source)", "Manual CSV upload"],
            key="rain_source"
        )
        
        if rain_source == "Auto-load from CSV (‚Üí Live source)":
            st.caption("‚úì Connected to weather stations")
            st.button("üîÑ Load Rainfall Data", key="load_rain")
        else:
            st.file_uploader("Upload rainfall data CSV", type=['csv'], key="rain_upload")
    
    st.markdown("---")
    
    # Initialize session state for integration
    if 'integration_completed' not in st.session_state:
        st.session_state.integration_completed = False
    if 'integration_data' not in st.session_state:
        st.session_state.integration_data = None
    if 'integration_report' not in st.session_state:
        st.session_state.integration_report = None
    
    # Load and integrate data
    if st.button("üîó INTEGRATE DATA", type="primary", use_container_width=True):
        with st.spinner("Loading and integrating data sources..."):
            try:
                # Load from auto-load sources
                health_df, health_error = load_health_data()
                water_df, water_error = load_water_data()
                rain_df, rain_error = load_rainfall_data()
                
                if health_error or water_error or rain_error:
                    st.error(f"Load errors: {health_error or ''} {water_error or ''} {rain_error or ''}")
                else:
                    # Integrate
                    integrated_df, report = integrate_data(health_df, water_df, rain_df)
                    
                    if integrated_df is None:
                        st.error(f"Integration failed: {report.get('error', 'Unknown error')}")
                    else:
                        st.session_state.integration_data = integrated_df
                        st.session_state.integration_report = report
                        st.session_state.integration_completed = True
                        st.success("‚úÖ Data integration successful!")
                        
            except Exception as e:
                st.error(f"‚ùå Integration error: {str(e)}")
    
    # Display integration results
    if st.session_state.integration_completed and st.session_state.integration_report:
        report = st.session_state.integration_report
        
        st.markdown("---")
        st.subheader("üìà Integration Results")
        
        # Key metrics
        col1, col2, col3, col4 = st.columns(4)
        
        col1.metric(
            "üìä Rows Merged",
            f"{report.get('total_rows', 0):,}",
            help="Total records after integration"
        )
        
        col2.metric(
            "üìç Wards Covered",
            f"{report.get('unique_wards', 0)}",
            help="Unique wards in integrated dataset"
        )
        
        col3.metric(
            "üó∫Ô∏è Zones Covered",
            f"{report.get('unique_zones', 0)}",
            help="Unique zones in integrated dataset"
        )
        
        col4.metric(
            "üìÖ Date Range",
            f"{(report.get('date_range_end') - report.get('date_range_start')).days + 1 if report.get('date_range_start') and report.get('date_range_end') else 0} days"
        )
        
        # Date range details
        st.markdown("#### üìÖ Date Range Alignment")
        col1, col2 = st.columns(2)
        col1.write(f"**Start Date:** {report.get('date_range_start')}")
        col2.write(f"**End Date:** {report.get('date_range_end')}")
        
        # Missing values analysis
        st.markdown("#### ‚ö†Ô∏è Data Quality - Missing Values")
        
        tab1, tab2, tab3, tab4 = st.tabs(["Health", "Water", "Rainfall", "Integrated"])
        
        with tab1:
            st.markdown(f"**Source Rows:** {report.get('health_rows', 0):,}")
            if report.get('missing_health'):
                missing_health = {k: v for k, v in report['missing_health'].items() if v > 0}
                if missing_health:
                    st.dataframe(pd.DataFrame([missing_health]))
                else:
                    st.success("‚úì No missing values in health data")
        
        with tab2:
            st.markdown(f"**Source Rows:** {report.get('water_rows', 0):,}")
            if report.get('missing_water'):
                missing_water = {k: v for k, v in report['missing_water'].items() if v > 0}
                if missing_water:
                    st.dataframe(pd.DataFrame([missing_water]))
                else:
                    st.success("‚úì No missing values in water data")
        
        with tab3:
            st.markdown(f"**Source Rows:** {report.get('rain_rows', 0):,}")
            if report.get('missing_rain'):
                missing_rain = {k: v for k, v in report['missing_rain'].items() if v > 0}
                if missing_rain:
                    st.dataframe(pd.DataFrame([missing_rain]))
                else:
                    st.success("‚úì No missing values in rainfall data")
        
        with tab4:
            st.markdown(f"**Integrated Rows:** {report.get('total_rows', 0):,}")
            if report.get('missing_integrated'):
                missing_integrated = {k: v for k, v in report['missing_integrated'].items() if v > 0}
                if missing_integrated:
                    st.dataframe(pd.DataFrame([missing_integrated]))
                else:
                    st.success("‚úì No missing values in integrated dataset")
        
        # Data preview
        st.markdown("#### üìã Integrated Data Preview")
        if st.session_state.integration_data is not None:
            display_cols = ['week_start_date', 'ward_id', 'zone', 'reported_cases', 'turbidity', 'ecoli_index', 'rainfall_mm']
            display_cols = [col for col in display_cols if col in st.session_state.integration_data.columns]
            
            st.dataframe(
                st.session_state.integration_data[display_cols].head(20),
                use_container_width=True,
                hide_index=True
            )
            
            st.info(f"Showing first 20 rows of {len(st.session_state.integration_data)} total rows")
        
        # Summary statistics
        st.markdown("#### üìä Summary Statistics")
        integrated = st.session_state.integration_data
        if integrated is not None:
            numeric_cols = integrated.select_dtypes(include=[np.number]).columns
            st.dataframe(integrated[numeric_cols].describe())


# =============================================================================
# PAGE ROUTING
# =============================================================================

def main():
    """Main app router"""
    # Sidebar navigation
    st.sidebar.title("üì± Navigation")
    page = st.sidebar.radio(
        "Select Page",
        [
            "üè† Operational Monitoring",
            "üîó Data Integration",
            "üß† Model Intelligence",
            "üìà Temporal Monitoring",
            "üî¨ Environmental Analysis",
            "‚öôÔ∏è Administration"
        ],
        index=0
    )

    st.sidebar.markdown("---")
    st.sidebar.caption(f"üè• Disease Outbreak Early Warning System ‚Äî Artifact {ARTIFACT_VERSION_LABEL}")
    st.sidebar.caption(f"{OPERATIONAL_MODE_LABEL} ‚Äî Auto-Predict ‚Üí Auto-Alert ‚Üí Monitor + Intervene")

    if page == "üè† Operational Monitoring":
        page_operational_monitoring()
    elif page == "üîó Data Integration":
        page_data_integration()
    elif page == "üß† Model Intelligence":
        page_model_intelligence()
    elif page == "üìà Temporal Monitoring":
        page_temporal_monitoring()
    elif page == "üî¨ Environmental Analysis":
        page_environmental_analysis()
    elif page == "‚öôÔ∏è Administration":
        page_admin()


if __name__ == "__main__":
    main()
