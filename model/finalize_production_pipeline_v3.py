"""Production pipeline freeze script for outbreak prediction model v3.

Phases covered:
1) Strict data validation
2) Multi-source integration
3) Target generation/merge
4) Leakage-safe feature engineering
5) TimeSeries CV with threshold tuning
6) Calibration from OOF predictions
7) Final deployment model training + artifact freeze
8) Operational verification simulation
9) Stress tests
10) Final readiness report
"""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import joblib
import numpy as np
import pandas as pd
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import TimeSeriesSplit
from xgboost import XGBClassifier

from utils.feature_engineering import engineer_outbreak_features, get_model_feature_columns
from utils.constants import GLOBAL_THRESHOLD


DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data")
MODEL_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "model")
_workspace_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
_repo_doc_dir = os.path.join(_workspace_root, "doc")
DOC_DIR = _repo_doc_dir if os.path.isdir(_repo_doc_dir) else os.path.join(os.path.dirname(os.path.dirname(__file__)), "doc")


def _path(name: str) -> str:
    return os.path.join(DATA_DIR, name)


def _assert_required_columns(df: pd.DataFrame, required: List[str], dataset_name: str) -> None:
    missing = [column for column in required if column not in df.columns]
    if missing:
        raise ValueError(f"[{dataset_name}] missing required columns: {missing}")


def _ward_id_valid(series: pd.Series) -> bool:
    return series.astype(str).str.fullmatch(r"W(0[1-9]|[1-9][0-9]|100)").all()


def _strict_read_csv(file_name: str) -> pd.DataFrame:
    file_path = _path(file_name)
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Required dataset not found: {file_path}")
    return pd.read_csv(file_path)


@dataclass
class DatasetSummary:
    name: str
    total_rows: int
    total_weeks: int
    total_wards: int
    date_start: str
    date_end: str
    missing_pct: Dict[str, float]


class ProductionPipelineV3:
    def __init__(self) -> None:
        self.feature_columns = get_model_feature_columns()
        self.global_threshold = float(GLOBAL_THRESHOLD)
        self.best_params: Dict[str, float] = {}
        self.calibration_obj: Dict[str, object] = {"method": "none"}
        self.cv_metrics: Dict[str, object] = {}
        self.validation_summary: Dict[str, object] = {}
        self.integration_summary: Dict[str, object] = {}
        self.operational_summary: Dict[str, object] = {}
        self.stress_summary: Dict[str, object] = {}
        self.blocking_issues: List[str] = []

    def phase1_strict_validation(self) -> Dict[str, object]:
        required = {
            "health_surveillance_weekly.csv": ["week_start_date", "ward_id", "reported_cases"],
            "water_quality_weekly.csv": ["week_start_date", "ward_id", "turbidity", "ecoli_index"],
            "rainfall_environment_weekly.csv": ["week_start_date", "ward_id", "rainfall_mm"],
            "ward_metadata.csv": ["ward_id", "zone"],
        }

        labels_path = _path("outbreak_labels_weekly.csv")
        labels_present = os.path.exists(labels_path)
        if labels_present:
            required["outbreak_labels_weekly.csv"] = ["week_start_date", "ward_id", "outbreak_next_week"]

        loaded: Dict[str, pd.DataFrame] = {}
        summaries: List[DatasetSummary] = []

        for name, cols in required.items():
            df = _strict_read_csv(name)
            _assert_required_columns(df, cols, name)
            if "week_start_date" in df.columns:
                df["week_start_date"] = pd.to_datetime(df["week_start_date"], errors="coerce")
                if df["week_start_date"].isna().any():
                    raise ValueError(f"[{name}] contains invalid week_start_date values")

            if "ward_id" in df.columns and not _ward_id_valid(df["ward_id"]):
                bad = sorted(df.loc[~df["ward_id"].astype(str).str.fullmatch(r"W(0[1-9]|[1-9][0-9]|100)"), "ward_id"].astype(str).unique().tolist())
                raise ValueError(f"[{name}] invalid ward_id values detected: {bad[:10]}")

            if {"ward_id", "week_start_date"}.issubset(df.columns):
                dup_count = int(df.duplicated(["ward_id", "week_start_date"]).sum())
                if dup_count > 0:
                    raise ValueError(f"[{name}] duplicate keys detected for (ward_id, week_start_date): {dup_count}")

            critical_cols = [column for column in cols if column in df.columns]
            missing_pct = {column: float(df[column].isna().mean() * 100.0) for column in critical_cols}
            severe_missing = {column: value for column, value in missing_pct.items() if value > 5.0}
            if severe_missing:
                raise ValueError(f"[{name}] severe missing values (>5%) in critical columns: {severe_missing}")

            total_weeks = int(df["week_start_date"].nunique()) if "week_start_date" in df.columns else 0
            total_wards = int(df["ward_id"].nunique()) if "ward_id" in df.columns else 0
            date_start = str(df["week_start_date"].min().date()) if "week_start_date" in df.columns else "N/A"
            date_end = str(df["week_start_date"].max().date()) if "week_start_date" in df.columns else "N/A"

            summaries.append(
                DatasetSummary(
                    name=name,
                    total_rows=int(len(df)),
                    total_weeks=total_weeks,
                    total_wards=total_wards,
                    date_start=date_start,
                    date_end=date_end,
                    missing_pct=missing_pct,
                )
            )

            loaded[name] = df

        core_names = [
            "health_surveillance_weekly.csv",
            "water_quality_weekly.csv",
            "rainfall_environment_weekly.csv",
        ]
        core_ranges = {
            name: (
                str(loaded[name]["week_start_date"].min().date()),
                str(loaded[name]["week_start_date"].max().date()),
            )
            for name in core_names
        }
        if len(set(core_ranges.values())) != 1:
            raise ValueError(f"Core source date range mismatch: {core_ranges}")

        labels_range_check = {"status": "not_present"}
        if labels_present:
            labels_df = loaded["outbreak_labels_weekly.csv"]
            labels_range = (str(labels_df["week_start_date"].min().date()), str(labels_df["week_start_date"].max().date()))
            core_range = list(core_ranges.values())[0]
            if labels_range[0] != core_range[0] or labels_range[1] > core_range[1]:
                raise ValueError(
                    "Label date range invalid. Expected same start date and end date <= core datasets. "
                    f"labels={labels_range}, core={core_range}"
                )
            labels_range_check = {
                "status": "subset_allowed",
                "labels_range": labels_range,
                "core_range": core_range,
            }

        for name, df in loaded.items():
            if {"ward_id", "week_start_date"}.issubset(df.columns):
                ward_counts = df.groupby("week_start_date")["ward_id"].nunique()
                bad_weeks = ward_counts[ward_counts != 100]
                if not bad_weeks.empty:
                    raise ValueError(
                        f"[{name}] each week must contain exactly 100 wards. Violations: {bad_weeks.head(10).to_dict()}"
                    )

        self.validation_summary = {
            "datasets": [summary.__dict__ for summary in summaries],
            "core_date_range": list(core_ranges.values())[0],
            "core_ranges": core_ranges,
            "labels_range_check": labels_range_check,
            "labels_present": labels_present,
        }

        return loaded

    def phase2_integration(self, loaded: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        health = loaded["health_surveillance_weekly.csv"].copy()
        water = loaded["water_quality_weekly.csv"].copy()
        rain = loaded["rainfall_environment_weekly.csv"].copy()
        ward = loaded["ward_metadata.csv"].copy()

        before_rows = len(health)
        merged = health.merge(water, on=["ward_id", "week_start_date"], how="inner", validate="one_to_one")
        merged = merged.merge(rain, on=["ward_id", "week_start_date"], how="inner", validate="one_to_one")
        merged = merged.merge(ward[["ward_id", "zone"]], on="ward_id", how="left", validate="many_to_one")

        if len(merged) != before_rows:
            raise ValueError(f"Row loss detected after integration. Before={before_rows}, After={len(merged)}")

        ward_counts = merged.groupby("week_start_date")["ward_id"].nunique()
        if (ward_counts != 100).any():
            raise ValueError("Integrated dataset does not retain 100 wards per week.")

        if merged["zone"].isna().any():
            raise ValueError("Zone mapping failed for one or more wards.")

        final_path = _path("integrated_surveillance_dataset_final.csv")
        merged.to_csv(final_path, index=False)

        self.integration_summary = {
            "rows": int(len(merged)),
            "weeks": int(merged["week_start_date"].nunique()),
            "wards": int(merged["ward_id"].nunique()),
            "saved_path": final_path,
        }

        return merged

    def phase3_target_generation(self, integrated: pd.DataFrame, loaded: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        labels_present = "outbreak_labels_weekly.csv" in loaded
        data = integrated.copy()

        if labels_present:
            labels = loaded["outbreak_labels_weekly.csv"][["week_start_date", "ward_id", "outbreak_next_week"]].copy()
            labels["outbreak_next_week"] = pd.to_numeric(labels["outbreak_next_week"], errors="coerce").fillna(0).astype(int)
            data = data.merge(labels, on=["week_start_date", "ward_id"], how="left", validate="one_to_one")
            if data["outbreak_next_week"].isna().any():
                max_labeled_week = pd.to_datetime(labels["week_start_date"]).max()
                missing_mask = data["outbreak_next_week"].isna()
                missing_weeks = pd.to_datetime(data.loc[missing_mask, "week_start_date"]).dropna()
                if (missing_weeks <= max_labeled_week).any():
                    raise ValueError(
                        "Unexpected missing labels found within labeled date range. "
                        f"max_labeled_week={str(max_labeled_week.date())}"
                    )
                data = data.loc[~missing_mask].copy()
            data["outbreak_next_week"] = data["outbreak_next_week"].astype(int)
        else:
            threshold = float(data["reported_cases"].quantile(0.75))
            data = data.sort_values(["ward_id", "week_start_date"]).reset_index(drop=True)
            data["next_week_cases"] = data.groupby("ward_id")["reported_cases"].shift(-1)
            data["prev_cases"] = data.groupby("ward_id")["reported_cases"].shift(1)
            growth = (data["reported_cases"] - data["prev_cases"]) / (data["prev_cases"].replace(0, np.nan) + 1.0)
            contamination_spike = (data["ecoli_index"] > data["ecoli_index"].quantile(0.85)) & (
                data["turbidity"] > data["turbidity"].quantile(0.85)
            )
            data["outbreak_next_week"] = (
                (data["next_week_cases"] > threshold) | ((growth > 0.5) & contamination_spike)
            ).astype(int)

        data = data.sort_values(["ward_id", "week_start_date"]).reset_index(drop=True)
        data["next_week_cases_shift_check"] = data.groupby("ward_id")["reported_cases"].shift(-1)
        data = data.dropna(subset=["next_week_cases_shift_check"]).drop(columns=["next_week_cases_shift_check"], errors="ignore")

        overall_rate = float(data["outbreak_next_week"].mean())
        per_week_rate = (
            data.groupby("week_start_date")["outbreak_next_week"].mean().reset_index(name="outbreak_rate")
        )

        return data

    def phase4_feature_engineering(self, labeled: pd.DataFrame) -> pd.DataFrame:
        featured = engineer_outbreak_features(labeled, dropna_lag_rows=True, require_target=True)

        required_features = [
            "cases_last_week",
            "cases_2w_avg",
            "rainfall_2w_avg",
            "rainfall_3w_avg",
            "rainfall_acceleration",
            "ecoli_last_week",
            "contamination_growth_rate",
            "rainfall_2w_avg_turbidity_interaction",
            "monsoon_flag",
            "outbreak_last_week",
        ]
        missing = [column for column in required_features if column not in featured.columns]
        if missing:
            raise ValueError(f"Feature engineering missing required outputs: {missing}")

        if featured[self.feature_columns].isna().any().any():
            raise ValueError("NaN found in model feature columns after feature engineering.")

        return featured

    @staticmethod
    def _eval_metrics(y_true: np.ndarray, y_proba: np.ndarray, threshold: float) -> Dict[str, float]:
        y_pred = (y_proba >= threshold).astype(int)
        roc = np.nan
        if len(np.unique(y_true)) > 1:
            roc = roc_auc_score(y_true, y_proba)
        return {
            "recall": float(recall_score(y_true, y_pred, zero_division=0)),
            "precision": float(precision_score(y_true, y_pred, zero_division=0)),
            "f1": float(f1_score(y_true, y_pred, zero_division=0)),
            "roc_auc": float(roc) if not pd.isna(roc) else np.nan,
        }

    def _tune_threshold(self, y_true: np.ndarray, y_proba: np.ndarray) -> Tuple[float, Dict[str, float]]:
        best_threshold = 0.5
        best_metrics = self._eval_metrics(y_true, y_proba, best_threshold)
        best_score = (0.7 * best_metrics["recall"]) + (0.3 * best_metrics["f1"])
        for threshold in np.arange(0.2, 0.81, 0.02):
            metrics = self._eval_metrics(y_true, y_proba, float(threshold))
            score = (0.7 * metrics["recall"]) + (0.3 * metrics["f1"])
            if score > best_score:
                best_score = score
                best_threshold = float(threshold)
                best_metrics = metrics
        return best_threshold, best_metrics

    def phase5_timeseries_cv(self, featured: pd.DataFrame) -> Tuple[XGBClassifier, np.ndarray, np.ndarray]:
        featured = featured.sort_values(["week_start_date", "ward_id"]).reset_index(drop=True)
        unique_dates = np.array(sorted(pd.to_datetime(featured["week_start_date"]).unique()))
        if len(unique_dates) < 8:
            raise ValueError("Insufficient temporal coverage for 5-fold TimeSeriesSplit.")

        n_splits = 5
        tscv = TimeSeriesSplit(n_splits=n_splits)

        param_grid = [
            {"max_depth": 3, "learning_rate": 0.06, "n_estimators": 220, "subsample": 0.9, "colsample_bytree": 0.9, "min_child_weight": 2, "reg_lambda": 1.0},
            {"max_depth": 4, "learning_rate": 0.04, "n_estimators": 280, "subsample": 0.85, "colsample_bytree": 0.85, "min_child_weight": 3, "reg_lambda": 1.5},
            {"max_depth": 5, "learning_rate": 0.03, "n_estimators": 320, "subsample": 0.8, "colsample_bytree": 0.8, "min_child_weight": 3, "reg_lambda": 2.0},
        ]

        best_bundle = None
        all_bundles = []

        for params in param_grid:
            fold_metrics = []
            fold_thresholds = []
            oof_rows = []

            for fold_id, (train_idx, val_idx) in enumerate(tscv.split(unique_dates), start=1):
                train_dates = set(unique_dates[train_idx])
                val_dates = set(unique_dates[val_idx])
                train_df = featured[featured["week_start_date"].isin(train_dates)]
                val_df = featured[featured["week_start_date"].isin(val_dates)]

                y_train = train_df["outbreak_next_week"].astype(int).to_numpy()
                y_val = val_df["outbreak_next_week"].astype(int).to_numpy()

                if len(np.unique(y_train)) < 2 or len(np.unique(y_val)) < 2:
                    fold_metrics.append({"fold": fold_id, "skipped": True, "reason": "single_class"})
                    continue

                neg = int((y_train == 0).sum())
                pos = int((y_train == 1).sum())
                scale_pos_weight = max(1.0, (neg / pos)) if pos > 0 else 1.0

                model = XGBClassifier(
                    objective="binary:logistic",
                    eval_metric="logloss",
                    random_state=42,
                    max_depth=params["max_depth"],
                    learning_rate=params["learning_rate"],
                    n_estimators=params["n_estimators"],
                    subsample=params["subsample"],
                    colsample_bytree=params["colsample_bytree"],
                    min_child_weight=params["min_child_weight"],
                    reg_lambda=params["reg_lambda"],
                    scale_pos_weight=scale_pos_weight,
                )
                model.fit(train_df[self.feature_columns], y_train, verbose=False)
                val_proba = model.predict_proba(val_df[self.feature_columns])[:, 1]

                tuned_threshold, metrics = self._tune_threshold(y_val, val_proba)
                metrics.update({"fold": fold_id, "skipped": False, "threshold": float(tuned_threshold)})
                fold_metrics.append(metrics)
                fold_thresholds.append(float(tuned_threshold))

                oof_rows.append(
                    pd.DataFrame(
                        {
                            "row_index": val_df.index.to_numpy(),
                            "y_true": y_val,
                            "y_proba": val_proba,
                        }
                    )
                )

            valid = [record for record in fold_metrics if not record.get("skipped", False)]
            if not valid:
                continue

            bundle_score = float(np.mean([(0.5 * record["recall"]) + (0.3 * record["roc_auc"]) + (0.2 * record["f1"]) for record in valid]))
            oof_df = pd.concat(oof_rows, ignore_index=True) if oof_rows else pd.DataFrame(columns=["row_index", "y_true", "y_proba"])

            bundle = {
                "params": params,
                "fold_metrics": fold_metrics,
                "fold_thresholds": fold_thresholds,
                "score": bundle_score,
                "oof": oof_df,
            }
            all_bundles.append(bundle)
            if best_bundle is None or bundle["score"] > best_bundle["score"]:
                best_bundle = bundle

        if best_bundle is None:
            raise ValueError("All CV folds were invalid (single-class). Cannot proceed.")

        valid_folds = [record for record in best_bundle["fold_metrics"] if not record.get("skipped", False)]
        if len(valid_folds) < 2:
            raise ValueError("Insufficient valid folds after skipping single-class folds.")

        recalls = np.array([record["recall"] for record in valid_folds], dtype=float)
        f1s = np.array([record["f1"] for record in valid_folds], dtype=float)
        rocs = np.array([record["roc_auc"] for record in valid_folds], dtype=float)

        self.global_threshold = float(GLOBAL_THRESHOLD)
        self.best_params = dict(best_bundle["params"])
        self.cv_metrics = {
            "fold_metrics": best_bundle["fold_metrics"],
            "thresholds": [float(value) for value in best_bundle["fold_thresholds"]],
            "recall_mean": float(np.mean(recalls)),
            "recall_std": float(np.std(recalls)),
            "f1_mean": float(np.mean(f1s)),
            "f1_std": float(np.std(f1s)),
            "roc_auc_mean": float(np.mean(rocs)),
            "roc_auc_std": float(np.std(rocs)),
            "global_threshold": float(self.global_threshold),
        }

        return None, best_bundle["oof"]["y_true"].to_numpy(dtype=int), best_bundle["oof"]["y_proba"].to_numpy(dtype=float)

    def phase6_calibration(self, y_oof: np.ndarray, p_oof: np.ndarray) -> None:
        if len(y_oof) == 0:
            raise ValueError("No out-of-fold predictions available for calibration.")

        try:
            if len(np.unique(y_oof)) > 1:
                calibrator = IsotonicRegression(out_of_bounds="clip")
                calibrator.fit(p_oof, y_oof)
                self.calibration_obj = {"method": "isotonic", "calibrator": calibrator, "fitted_on": "oof"}
            else:
                raise ValueError("single-class oof")
        except Exception:
            alpha = 0.1
            self.calibration_obj = {
                "method": "smoothing",
                "alpha": float(alpha),
                "target_rate": float(np.mean(y_oof)),
                "fitted_on": "oof",
            }

        if "method" not in self.calibration_obj:
            raise ValueError("Calibration object creation failed.")

    def _apply_calibration(self, probabilities: np.ndarray) -> np.ndarray:
        method = self.calibration_obj.get("method", "none")
        if method == "isotonic":
            return np.clip(self.calibration_obj["calibrator"].transform(probabilities), 0.0, 1.0)
        if method == "smoothing":
            alpha = float(self.calibration_obj.get("alpha", 0.1))
            target_rate = float(self.calibration_obj.get("target_rate", 0.0))
            return np.clip(((1.0 - alpha) * probabilities) + (alpha * target_rate), 0.0, 1.0)
        return probabilities

    def phase7_final_model(self, featured: pd.DataFrame) -> str:
        y = featured["outbreak_next_week"].astype(int).to_numpy()
        neg = int((y == 0).sum())
        pos = int((y == 1).sum())
        scale_pos_weight = max(1.0, (neg / pos)) if pos > 0 else 1.0

        final_model = XGBClassifier(
            objective="binary:logistic",
            eval_metric="logloss",
            random_state=42,
            max_depth=self.best_params["max_depth"],
            learning_rate=self.best_params["learning_rate"],
            n_estimators=self.best_params["n_estimators"],
            subsample=self.best_params["subsample"],
            colsample_bytree=self.best_params["colsample_bytree"],
            min_child_weight=self.best_params["min_child_weight"],
            reg_lambda=self.best_params["reg_lambda"],
            scale_pos_weight=scale_pos_weight,
        )
        final_model.fit(featured[self.feature_columns], y, verbose=False)

        training_range = {
            "start": str(pd.to_datetime(featured["week_start_date"]).min().date()),
            "end": str(pd.to_datetime(featured["week_start_date"]).max().date()),
        }

        artifact = {
            "model": final_model,
            "feature_columns": self.feature_columns,
            "global_threshold": float(self.global_threshold),
            "calibration": self.calibration_obj,
            "cv_metrics": self.cv_metrics,
            "training_data_range": training_range,
            "artifact_version": "3.0-final",
            "retraining_frequency_days": 30,
            "best_params": self.best_params,
            "generated_at_utc": datetime.utcnow().isoformat() + "Z",
        }

        artifact_path = os.path.join(MODEL_DIR, "final_outbreak_model_v3.pkl")
        joblib.dump(artifact, artifact_path)

        return artifact_path

    def phase8_operational_verification(self, integrated: pd.DataFrame, artifact_path: str) -> Dict[str, object]:
        artifact = joblib.load(artifact_path)
        model = artifact["model"]
        threshold = float(artifact["global_threshold"])
        feature_cols = artifact["feature_columns"]

        latest_week = pd.to_datetime(integrated["week_start_date"]).max()
        latest_data = integrated[integrated["week_start_date"] <= latest_week].copy()
        featured_latest = engineer_outbreak_features(latest_data, dropna_lag_rows=True, require_target=False)
        latest_rows = featured_latest.sort_values("week_start_date").groupby("ward_id", as_index=False).tail(1)

        if len(latest_rows) != 100:
            raise ValueError(f"Operational check failed: expected 100 wards, found {len(latest_rows)}")

        proba = model.predict_proba(latest_rows[feature_cols])[:, 1]
        proba = self._apply_calibration(proba)
        pred = (proba >= threshold).astype(int)

        risk = np.where(proba >= threshold, "High", np.where(proba >= (threshold * 0.7), "Medium", "Low"))
        alerts = int((risk == "High").sum())

        self.operational_summary = {
            "predicted_wards": int(len(latest_rows)),
            "high_risk_count": alerts,
            "threshold_used": threshold,
            "example_ward": str(latest_rows.iloc[0]["ward_id"]),
            "example_probability": float(proba[0]),
            "retraining_required": False,
        }

        return self.operational_summary

    def phase9_stress_test(self, integrated: pd.DataFrame, artifact_path: str) -> Dict[str, object]:
        artifact = joblib.load(artifact_path)
        model = artifact["model"]
        threshold = float(artifact["global_threshold"])

        results = {}

        def run_sim(df: pd.DataFrame, label: str) -> str:
            fe = engineer_outbreak_features(df, dropna_lag_rows=True, require_target=False)
            latest = fe.sort_values("week_start_date").groupby("ward_id", as_index=False).tail(1)
            _ = model.predict_proba(latest[self.feature_columns])[:, 1]
            _ = threshold
            return "pass"

        high_rain = integrated.copy()
        high_rain["rainfall_mm"] = high_rain["rainfall_mm"] * 3.0
        results["high_rainfall_spike"] = run_sim(high_rain, "high_rainfall_spike")

        zero_rain = integrated.copy()
        zero_rain["rainfall_mm"] = 0.0
        results["zero_rainfall_week"] = run_sim(zero_rain, "zero_rainfall_week")

        try:
            missing_turbidity = integrated.drop(columns=["turbidity"]).copy()
            run_sim(missing_turbidity, "missing_turbidity_column")
            results["missing_turbidity_column"] = "unexpected_pass"
        except Exception as exc:
            results["missing_turbidity_column"] = f"graceful_error: {type(exc).__name__}"

        try:
            bad_ward = integrated.copy()
            bad_ward.loc[bad_ward.index[0], "ward_id"] = "BAD"
            run_sim(bad_ward, "corrupted_ward_id")
            results["corrupted_ward_id"] = "unexpected_pass"
        except Exception as exc:
            results["corrupted_ward_id"] = f"graceful_error: {type(exc).__name__}"

        self.stress_summary = results

        return results

    def phase10_report(self, artifact_path: str) -> str:
        ready = True

        recall_ok = self.cv_metrics.get("recall_mean", 0.0) >= 0.70
        roc_ok = self.cv_metrics.get("roc_auc_mean", 0.0) > 0.50
        op_ok = self.operational_summary.get("predicted_wards") == 100
        stress_ok = (
            str(self.stress_summary.get("missing_turbidity_column", "")).startswith("graceful_error")
            and str(self.stress_summary.get("corrupted_ward_id", "")).startswith("graceful_error")
        )

        if not recall_ok:
            ready = False
            self.blocking_issues.append("Recall target not met: mean recall <= 70%")
        if not roc_ok:
            ready = False
            self.blocking_issues.append("ROC-AUC target not met: mean ROC-AUC <= 0.50")
        if not op_ok:
            ready = False
            self.blocking_issues.append("Operational prediction did not produce 100 ward predictions")
        if not stress_ok:
            ready = False
            self.blocking_issues.append("Stress test graceful error handling failed")

        lines = []
        lines.append("# Final Readiness Report â€” Outbreak Pipeline v3")
        lines.append("")
        lines.append("## 1) Data validation summary")
        lines.append(json.dumps(self.validation_summary, indent=2, default=str))
        lines.append("")
        lines.append("## 2) Integration summary")
        lines.append(json.dumps(self.integration_summary, indent=2, default=str))
        lines.append("")
        lines.append("## 3) CV metrics")
        lines.append(json.dumps(self.cv_metrics, indent=2, default=str))
        lines.append("")
        lines.append("## 4) Global threshold")
        lines.append(str(self.global_threshold))
        lines.append("")
        lines.append("## 5) Calibration status")
        lines.append(json.dumps({k: v for k, v in self.calibration_obj.items() if k != "calibrator"}, indent=2, default=str))
        lines.append("")
        lines.append("## 6) Operational simulation result")
        lines.append(json.dumps(self.operational_summary, indent=2, default=str))
        lines.append("")
        lines.append("## 7) Final artifact confirmation")
        lines.append(json.dumps({"artifact_path": artifact_path, "exists": os.path.exists(artifact_path)}, indent=2, default=str))
        lines.append("")

        if ready:
            lines.append("## Final Declaration")
            lines.append("System Ready for Demo")
        else:
            lines.append("## Final Declaration")
            lines.append("Blocking issues:")
            for issue in self.blocking_issues:
                lines.append(f"- {issue}")

        os.makedirs(DOC_DIR, exist_ok=True)
        report_path = os.path.join(DOC_DIR, "FINAL_READINESS_REPORT_V3.md")
        with open(report_path, "w", encoding="utf-8") as file:
            file.write("\n".join(lines))

        return report_path

    def run(self) -> Dict[str, str]:
        loaded = self.phase1_strict_validation()
        integrated = self.phase2_integration(loaded)
        labeled = self.phase3_target_generation(integrated, loaded)
        featured = self.phase4_feature_engineering(labeled)
        _, y_oof, p_oof = self.phase5_timeseries_cv(featured)
        self.phase6_calibration(y_oof, p_oof)
        artifact_path = self.phase7_final_model(featured)
        self.phase8_operational_verification(labeled, artifact_path)
        self.phase9_stress_test(labeled, artifact_path)
        report_path = self.phase10_report(artifact_path)
        return {"artifact_path": artifact_path, "report_path": report_path}


def main() -> None:
    pipeline = ProductionPipelineV3()
    result = pipeline.run()


if __name__ == "__main__":
    main()
